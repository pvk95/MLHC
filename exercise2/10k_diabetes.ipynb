{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes readmission prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "from helper import EstimatorSelectionHelper\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 512  # Size of universal sentence encoder embeddings \n",
    "useDummy = True  # Whether to use 1hot encoding or ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid confusion we will only present the results achieved with 1hot encoding, which after experimentation seemed to offer better results. We also tried a couple of embeding sizes without noticing significant differences in performance. These among other experimentation insights will be listed at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('project2_data/10k_diabetes/diab_train.csv')\n",
    "valid_df = pd.read_csv('project2_data/10k_diabetes/diab_validation.csv')\n",
    "test_df = pd.read_csv('project2_data/10k_diabetes/diab_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns provide no information or contain too many nans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['Unnamed: 0', 'weight', 'payer_code', 'discharge_disposition_id', 'admission_source_id',\n",
    "             'diag_1', 'diag_2', 'diag_3', 'max_glu_serum', 'A1Cresult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values mapped to nan\n",
    "NaN_values = ['?', 'nan', 'Not Available', 'Not Mapped', 'None']\n",
    "\n",
    "# Prediction\n",
    "pred_columns = ['readmitted']\n",
    "\n",
    "# All integer columns\n",
    "int_columns = ['time_in_hospital', 'num_lab_procedures',\n",
    "               'num_procedures', 'num_medications', 'number_outpatient',\n",
    "               'number_emergency', 'number_inpatient', 'number_diagnoses',\n",
    "               ]\n",
    "\n",
    "# All categorical columns\n",
    "cat_columns = ['race', 'gender', 'age', 'metformin', 'repaglinide', 'nateglinide',\n",
    "               'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n",
    "               'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "               'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n",
    "               'insulin', 'glyburide.metformin', 'glipizide.metformin',\n",
    "               'glimepiride.pioglitazone', 'metformin.rosiglitazone',\n",
    "               'metformin.pioglitazone', 'change', 'diabetesMed'\n",
    "               ]\n",
    "\n",
    "\n",
    "# All string columns\n",
    "str_columns = ['diag_1_desc', 'diag_2_desc', 'diag_3_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary mapping each category to a number\n",
    "def create_maps(df, cat_columns, nan_categories):\n",
    "    maps = {}\n",
    "    for column in cat_columns:\n",
    "        maps[column] = {category: i for i,\n",
    "                        category in enumerate(df[column].unique())}\n",
    "\n",
    "    for key in maps.keys():\n",
    "        for category in maps[key]:\n",
    "            if category in nan_categories:\n",
    "                maps[key][category] = np.NaN\n",
    "    return maps\n",
    "\n",
    "\n",
    "def fix_integers(df, int_columns):\n",
    "    for column in int_columns:\n",
    "        df[column] = df[column].where(\n",
    "            lambda x: [str(i).isdigit() for i in x], np.NaN).astype(np.float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# encode categories into numbers using create_maps output\n",
    "def fix_categories(df, cat_columns, cat_map):\n",
    "    for column in cat_columns:\n",
    "        df[column] = df[column].replace(cat_map[column])\n",
    "        df[column] = df[column].astype('float64')\n",
    "    return df\n",
    "\n",
    "\n",
    "# map different nan strings to nan value\n",
    "def set_NaN(df, cat_columns, nan_values):\n",
    "    nan_map = {key: np.NaN for key in nan_values}\n",
    "    for column in cat_columns:\n",
    "        df[column] = df[column].replace(nan_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_missing_columns(df, unique_columns):\n",
    "    for column in unique_columns:\n",
    "        if column not in df.columns:\n",
    "            df[column] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_columns(df, to_remove):\n",
    "    return df.drop(columns=to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove uninteresting features\n",
    "train_df = remove_columns(train_df, to_remove)\n",
    "valid_df = remove_columns(valid_df, to_remove)\n",
    "test_df = remove_columns(test_df, to_remove)\n",
    "\n",
    "# Fix the Integers - Set all the unknown to 0\n",
    "train_df = fix_integers(train_df, int_columns)\n",
    "valid_df = fix_integers(valid_df, int_columns)\n",
    "test_df = fix_integers(test_df, int_columns)\n",
    "\n",
    "# Find category map\n",
    "# category_map = create_maps(pd.concat((train_df, valid_df, test_df)), cat_columns, NaN_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded result of create_maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {'race': {'AfricanAmerican': 0, 'Caucasian': 1, 'Asian': 2, 'Other': 3, 'Hispanic': 4, '?': np.NaN},\n",
    "                'gender': {'Male': 0, 'Female': 1},\n",
    "                'age': {'[0-10)': 0, '[10-20)': 1, '[20-30)': 2, '[30-40)': 3, '[40-50)': 4, '[50-60)': 5, '[60-70)': 6, '[70-80)': 7, '[80-90)': 8, '[90-100)': 9},\n",
    "                'weight': {'?': np.NaN, '[0-25)': 1, '[25-50)': 2, '[50-75)': 3, '[75-100)': 4, '[100-125)': 5, '[125-150)': 6, '[150-175)': 7},\n",
    "                'max_glu_serum': {'None': np.NaN, 'Norm': 1, '>200': 2, '>300': 3},\n",
    "                'A1Cresult': {'None': 0, '>8': 1, 'Norm': 2, '>7': 3},\n",
    "                'metformin': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'repaglinide': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'nateglinide': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'chlorpropamide': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'glimepiride': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'acetohexamide': {'No': 0},\n",
    "                'glipizide': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'glyburide': {'No': 0, 'Steady': 1, 'Down': 2, 'Up': 3},\n",
    "                'tolbutamide': {'No': 0, 'Steady': 1},\n",
    "                'pioglitazone': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'rosiglitazone': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'acarbose': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'miglitol': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'troglitazone': {'No': 0},\n",
    "                'tolazamide': {'No': 0, 'Steady': 1},\n",
    "                'examide': {'No': 0},\n",
    "                'citoglipton': {'No': 0},\n",
    "                'insulin': {'No': 0, 'Down': 1, 'Up': 2, 'Steady': 3},\n",
    "                'glyburide.metformin': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'glipizide.metformin': {'No': 0, 'Steady': 1, 'Up': 2, 'Down': 3},\n",
    "                'glimepiride.pioglitazone': {'No': 0},\n",
    "                'metformin.rosiglitazone': {'No': 0},\n",
    "                'metformin.pioglitazone': {'No': 0},\n",
    "                'change': {'No': 0, 'Ch': 1},\n",
    "                'diabetesMed': {'No': 0, 'Yes': 1},\n",
    "                'admission_type_id': {'Emergency': 0, 'Elective': 1, 'Urgent': 2, 'Newborn': 3, 'Not Available': np.NaN,\n",
    "                                      'Not Mapped': np.NaN},\n",
    "                'medical_specialty': {'?': np.NaN, 'Family/GeneralPractice': 1, 'Emergency/Trauma': 2, 'InternalMedicine': 3,\n",
    "                                      'Psychiatry': 4, 'Surgery-Neuro': 5, 'Surgery-General': 6, 'Cardiology': 7,\n",
    "                                      'Orthopedics': 8, 'Orthopedics-Reconstructive': 9, 'Nephrology': 10,\n",
    "                                      'Pediatrics-Pulmonology': 11, 'Gastroenterology': 12,\n",
    "                                      'Surgery-Cardiovascular/Thoracic': 13, 'Osteopath': 14,\n",
    "                                      'PhysicalMedicineandRehabilitation': 15, 'Hematology/Oncology': 16,\n",
    "                                      'Surgery-Vascular': 17, 'Pediatrics-Endocrinology': 18, 'Oncology': 19,\n",
    "                                      'ObstetricsandGynecology': 20, 'Urology': 21, 'Neurology': 22, 'Pulmonology': 23,\n",
    "                                      'Surgery-Cardiovascular': 24, 'Radiologist': 25, 'OutreachServices': 26,\n",
    "                                      'Surgery-Plastic': 27, 'Endocrinology': 28, 'Ophthalmology': 29,\n",
    "                                      'Obsterics&Gynecology-GynecologicOnco': 30, 'Radiology': 31,\n",
    "                                      'Surgery-Thoracic': 32, 'Pediatrics': 33, 'Psychology': 34, 'Otolaryngology': 35,\n",
    "                                      'InfectiousDiseases': 36, 'Pediatrics-CriticalCare': 37, 'Gynecology': 38,\n",
    "                                      'Pediatrics-Hematology-Oncology': 39, 'Surgeon': 40, 'Podiatry': 41,\n",
    "                                      'Obstetrics': 42, 'Anesthesiology-Pediatric': 43, 'Hospitalist': 44,\n",
    "                                      'Hematology': 45, 'Pathology': 46, 'Surgery-Pediatric': 47,\n",
    "                                      'Cardiology-Pediatric': 48, 'Surgery-Colon&Rectal': 49, 'PhysicianNotFound': 50,\n",
    "                                      'Surgery-PlasticwithinHeadandNeck': 51, 'Pediatrics-EmergencyMedicine': 52}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useDummy:\n",
    "    # Set all missing values to NaN\n",
    "    train_df = set_NaN(train_df, cat_columns, NaN_values)\n",
    "    valid_df = set_NaN(valid_df, cat_columns, NaN_values)\n",
    "    test_df = set_NaN(test_df, cat_columns, NaN_values)\n",
    "\n",
    "    # Create Dummy Variables for all the categories\n",
    "    train_cat_dummies = pd.get_dummies(train_df[cat_columns])\n",
    "    valid_cat_dummies = pd.get_dummies(valid_df[cat_columns])\n",
    "    test_cat_dummies = pd.get_dummies(test_df[cat_columns])\n",
    "\n",
    "    # Some dummy-variable get excluded because the category doesn't exist in a dataframe.\n",
    "    # In here we make it so that all 3 dataframes have the same columns\n",
    "    unique_columns = np.unique(np.concatenate(\n",
    "        (train_cat_dummies.columns,\n",
    "         valid_cat_dummies.columns,\n",
    "         test_cat_dummies.columns)))\n",
    "\n",
    "    train_cat_dummies = create_missing_columns(train_cat_dummies, unique_columns)\n",
    "    valid_cat_dummies = create_missing_columns(valid_cat_dummies, unique_columns)\n",
    "    test_cat_dummies = create_missing_columns(test_cat_dummies, unique_columns)\n",
    "\n",
    "    # Merge dummy variables and integer variables to one dataframe\n",
    "    train_data = pd.concat(\n",
    "        [train_cat_dummies, train_df[int_columns]], axis=1)\n",
    "    valid_data = pd.concat(\n",
    "        [valid_cat_dummies, valid_df[int_columns]], axis=1)\n",
    "    test_data = pd.concat(\n",
    "        [test_cat_dummies, test_df[int_columns]], axis=1)\n",
    "else:\n",
    "    # Fix categories - Apply category map to integers\n",
    "    train_data = fix_categories(train_df, cat_columns, category_map)[cat_columns + int_columns]\n",
    "    valid_data = fix_categories(valid_df, cat_columns, category_map)[cat_columns + int_columns]\n",
    "    test_data = fix_categories(test_df, cat_columns, category_map)[cat_columns + int_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get y_values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df[pred_columns[0]].values\n",
    "valid_y = valid_df[pred_columns[0]].values\n",
    "test_y = test_df[pred_columns[0]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values with median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.fillna(train_data.median())\n",
    "valid_data = valid_data.fillna(train_data.median())\n",
    "test_data = test_data.fillna(train_data.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model grid and evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC was commented out since it takes too long and the scores were not that great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(base_estimator=LogisticRegression(solver='liblinear', class_weight='balanced')),\n",
    "    # 'SVC': SVC(class_weight='balanced', gamma='auto'),\n",
    "    'LogisticRegression': LogisticRegression(solver='liblinear', class_weight='balanced'),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'RandomForest': RandomForestClassifier(class_weight='balanced')\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'AdaBoostClassifier':  {'n_estimators': [8, 16, 32, 64, 128, 256]},\n",
    "    # 'SVC': [\n",
    "    #    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]},\n",
    "    #     {'kernel': ['rbf'], 'C': [0.1, 1, 10, 100]},\n",
    "    # ],\n",
    "    'LogisticRegression': {'C': [0.1, 1, 10, 50, 100]},\n",
    "    'GaussianNB': {},\n",
    "    'BernoulliNB': {},\n",
    "    'RandomForest': {'n_estimators': [16, 32, 100]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following models prepend standardization and feature selection to the pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = {\n",
    "    'AdaBoostClassifier': Pipeline([('scaler', RobustScaler()),\n",
    "                                    ('k_best', SelectKBest()),\n",
    "                                    ('classifier', AdaBoostClassifier(base_estimator=LogisticRegression(solver='liblinear',\n",
    "                                                                                                        class_weight='balanced')))]),\n",
    "    'LogisticRegression': Pipeline([('scaler', RobustScaler()),\n",
    "                                    ('k_best', SelectKBest()),\n",
    "                                    ('classifier', LogisticRegression(solver='liblinear', class_weight='balanced'))]),\n",
    "\n",
    "    'GaussianNB': Pipeline([('scaler', RobustScaler()),\n",
    "                            ('k_best', SelectKBest()),\n",
    "                            ('classifier', GaussianNB())]),\n",
    "\n",
    "    'BernoulliNB': Pipeline([('scaler', RobustScaler()),\n",
    "                            ('k_best', SelectKBest()),\n",
    "                            ('classifier', BernoulliNB())]),\n",
    "\n",
    "    'RandomForest': Pipeline([('scaler', RobustScaler()),\n",
    "                              ('k_best', SelectKBest()),\n",
    "                              ('classifier', RandomForestClassifier(class_weight='balanced'))])\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    'AdaBoostClassifier':  {'k_best__k': [10, 20, 'all'],\n",
    "                            'k_best__score_func': [f_classif, mutual_info_classif],\n",
    "                            'classifier__n_estimators': [8, 16, 32, 64, 128, 256]},\n",
    "\n",
    "    'LogisticRegression':  {'k_best__k': [10, 20, 'all'],\n",
    "                            'k_best__score_func': [f_classif, mutual_info_classif],\n",
    "                            'classifier__C': [0.1, 1, 10, 50, 100]},\n",
    "\n",
    "    'GaussianNB':  {'k_best__k': [10, 20, 'all'],\n",
    "                    'k_best__score_func': [f_classif, mutual_info_classif]},\n",
    "\n",
    "    'BernoulliNB':  {'k_best__k': [10, 20, 'all'],\n",
    "                     'k_best__score_func': [f_classif, mutual_info_classif]},\n",
    "\n",
    "    'RandomForest':  {'k_best__k': [10, 20, 'all'],\n",
    "                      'k_best__score_func': [f_classif, mutual_info_classif],\n",
    "                      'classifier__n_estimators': [16, 32, 100]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of evaluation function. It performs 5x cross validation optimizing for f1 score. One can inspect the values of the grid search by checking the summary returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_train, train_y, X_test, test_y, models, params):\n",
    "    helper = EstimatorSelectionHelper(models, params)\n",
    "    helper.fit(X_train, train_y, X_test, test_y,\n",
    "               scoring=make_scorer(f1_score), n_jobs=-1)\n",
    "    summary = helper.score_summary(sort_by='mean_score')\n",
    "    # summary.to_pickle('summary')\n",
    "\n",
    "    print('\\nF1 test scores:')\n",
    "    sortedKeysAndValues = sorted(helper.test_f1_scores.items(), key=lambda kv: -kv[1])\n",
    "    for k, v in sortedKeysAndValues:\n",
    "        print(k + ': ' + str(v))\n",
    "\n",
    "    print('\\nAUROC test scores:')\n",
    "    sortedKeysAndValues = sorted(helper.test_auroc_scores.items(), key=lambda kv: -kv[1])\n",
    "    for k, v in sortedKeysAndValues:\n",
    "        print(k + ': ' + str(v))\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignoring text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to have a baseline score for the task when not using any information coming from the diagnosis columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   34.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GaussianNB.\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for BernoulliNB.\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForest.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 test scores:\n",
      "AdaBoostClassifier: 0.5862785862785863\n",
      "BernoulliNB: 0.5697373288938217\n",
      "GaussianNB: 0.5656855707106964\n",
      "LogisticRegression: 0.5459518599562363\n",
      "RandomForest: 0.3800829875518672\n",
      "\n",
      "AUROC test scores:\n",
      "AdaBoostClassifier: 0.6366059273824088\n",
      "LogisticRegression: 0.628508981341502\n",
      "RandomForest: 0.6056641010666028\n",
      "BernoulliNB: 0.5880488031668984\n",
      "GaussianNB: 0.497675915294452\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data\n",
    "X_valid = valid_data\n",
    "X_test = test_data\n",
    "\n",
    "X_train = np.concatenate((X_train, X_valid))\n",
    "y_train = np.concatenate((train_y, valid_y))\n",
    "y_test = test_y\n",
    "\n",
    "summary = evaluate(X_train, y_train, X_test, y_test, models, params)\n",
    "# print(summary[['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded results for models2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 test scores:  \n",
    "GaussianNB: 0.5656855707106964  \n",
    "BernoulliNB: 0.5655105973025047  \n",
    "AdaBoostClassifier: 0.5654496883348175  \n",
    "LogisticRegression: 0.5220012055455092  \n",
    "RandomForest: 0.420249653259362  \n",
    "  \n",
    "AUROC test scores:  \n",
    "LogisticRegression: 0.6376099486914812  \n",
    "BernoulliNB: 0.6219144105788951  \n",
    "AdaBoostClassifier: 0.6130119490028219  \n",
    "RandomForest: 0.5808404316560292  \n",
    "GaussianNB: 0.497675915294452  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded results for models without dummy variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 test scores:  \n",
    "LogisticRegression: 0.5248484848484849  \n",
    "AdaBoostClassifier: 0.5136417556346381  \n",
    "RandomForest: 0.43959469992205763  \n",
    "BernoulliNB: 0.41346906812842593  \n",
    "GaussianNB: 0.28185328185328185  \n",
    "  \n",
    "AUROC test scores:  \n",
    "RandomForest: 0.6465468875861803  \n",
    "LogisticRegression: 0.6419718518812602  \n",
    "BernoulliNB: 0.6195015206587048  \n",
    "GaussianNB: 0.618141756107448  \n",
    "AdaBoostClassifier: 0.614639696348852  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded results for models2 without dummy variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 test scores:  \n",
    "GaussianNB: 0.5682551056968829  \n",
    "AdaBoostClassifier: 0.5411230856494612  \n",
    "LogisticRegression: 0.5177725118483413  \n",
    "RandomForest: 0.437094682230869  \n",
    "BernoulliNB: 0.4362264150943397  \n",
    "  \n",
    "AUROC test scores:  \n",
    "AdaBoostClassifier: 0.6467929302691007  \n",
    "BernoulliNB: 0.6371612211657304  \n",
    "LogisticRegression: 0.6353804154203464  \n",
    "GaussianNB: 0.6253856497041741  \n",
    "RandomForest: 0.5750936895014476  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google's Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will use a pretrained model to obtain sentence embeddings. The module comes in two flavors, one using a deep averaging network (DAN) encoder and a more powerful one with a Transformer encoder. We tried both and kept the first one, since it is lighter and the results were almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined three ways to combine the three sentence vectors obtained for each diagnosis: \n",
    "1. Average\n",
    "2. Weighted average: we thought it might be a good idea to place a higher weight in the first diagnosis, since their order is not arbitrary\n",
    "3. Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_diag(embs):\n",
    "    diag_features = []\n",
    "    embs = [np.zeros(EMB_SIZE) if np.isnan(x).all() else x for x in embs]\n",
    "    for i in range(len(embs)):\n",
    "        if i % 3 == 0:\n",
    "            flattened = np.array(embs[i:i+3]).reshape(-1)\n",
    "            diag_features.append(flattened)\n",
    "    return diag_features\n",
    "\n",
    "\n",
    "def average_diag(embs, weighted=False):\n",
    "    diag_features = []\n",
    "    weights = np.array([3, 2, 1])\n",
    "    for i in range(len(embs)):\n",
    "        if i % 3 == 0:\n",
    "            aux = embs[i:i+3]\n",
    "            nonans = []\n",
    "            for j in range(3):\n",
    "                if not np.isnan(aux[j]).all():\n",
    "                    nonans.append(j)\n",
    "            if weighted:\n",
    "                averaged = np.dot(itemgetter(*nonans)(weights), itemgetter(*nonans)(aux))\\\n",
    "                           / np.sum(itemgetter(*nonans)(weights))\n",
    "            else:\n",
    "                averaged = np.average(itemgetter(*nonans)(aux), axis=0)\n",
    "            diag_features.append(averaged)\n",
    "\n",
    "    return diag_features\n",
    "\n",
    "\n",
    "def avoid_bug(array):\n",
    "    converted = np.zeros((array.shape[0], EMB_SIZE))\n",
    "    for i in range(array.shape[0]):\n",
    "        converted[i] = array[i]\n",
    "\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_train = train_df.shape[0]\n",
    "size_valid = valid_df.shape[0]\n",
    "size_test = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be patient please, first execution takes a while for the model to be downloaded (~1GB). Have a look at the instructions inside 'preprocessing_uni.py' if facing problems executing the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('project2_data/10k_diabetes/uni_emb.pkl'):\n",
    "    p = subprocess.run([\"python\", \"preprocessing_uni.py\"], stdout=subprocess.PIPE)\n",
    "    print(p)\n",
    "\n",
    "with open('project2_data/10k_diabetes/uni_emb.pkl', 'rb') as input_file:\n",
    "    sentence_embs = pkl.load(input_file)\n",
    "\n",
    "embs = np.array(average_diag(sentence_embs, weighted=False))\n",
    "embs = avoid_bug(embs)\n",
    "\n",
    "embs_train_unweighted = embs[:size_train]\n",
    "embs_valid_unweighted = embs[size_train:(size_train + size_valid)]\n",
    "embs_test_unweighted = embs[(size_train + size_valid):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the performance of the models using only the text representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GaussianNB.\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.5s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for BernoulliNB.\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.4s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForest.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   29.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 test scores:\n",
      "AdaBoostClassifier: 0.5322503583373148\n",
      "GaussianNB: 0.49640685461580975\n",
      "LogisticRegression: 0.4954545454545455\n",
      "BernoulliNB: 0.472663139329806\n",
      "RandomForest: 0.3408071748878924\n",
      "\n",
      "AUROC test scores:\n",
      "LogisticRegression: 0.578699181215921\n",
      "GaussianNB: 0.5638854266463703\n",
      "BernoulliNB: 0.5638613970000554\n",
      "AdaBoostClassifier: 0.5635834889165868\n",
      "RandomForest: 0.5183842465817828\n"
     ]
    }
   ],
   "source": [
    "X_train = embs_train_unweighted\n",
    "X_valid = embs_valid_unweighted\n",
    "X_test = embs_test_unweighted\n",
    "\n",
    "X_train = np.concatenate((X_train, X_valid))\n",
    "y_train = np.concatenate((train_y, valid_y))\n",
    "y_test = test_y\n",
    "\n",
    "summary = evaluate(X_train, y_train, X_test, y_test, models, params)\n",
    "# print(summary[['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded results for models2+unweighted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 test scores:  \n",
    "LogisticRegression: 0.505849582172702  \n",
    "AdaBoostClassifier: 0.5042492917847025  \n",
    "GaussianNB: 0.48763657274295574  \n",
    "BernoulliNB: 0.44458052663808934  \n",
    "RandomForest: 0.319634703196347  \n",
    "  \n",
    "AUROC test scores:  \n",
    "LogisticRegression: 0.5771670300715352  \n",
    "AdaBoostClassifier: 0.5767470336446392  \n",
    "GaussianNB: 0.5674402471501362  \n",
    "BernoulliNB: 0.5494587583359365  \n",
    "RandomForest: 0.5154155404946555   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded results for models+weighted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 test scores:  \n",
    "LogisticRegression: 0.49520586576424136  \n",
    "AdaBoostClassifier: 0.49229074889867847  \n",
    "GaussianNB: 0.48763657274295574  \n",
    "BernoulliNB: 0.4520123839009288  \n",
    "RandomForest: 0.3214013709063214  \n",
    "  \n",
    "AUROC test scores:  \n",
    "LogisticRegression: 0.5732679587651269  \n",
    "GaussianNB: 0.5674402471501362  \n",
    "BernoulliNB: 0.5607411996644207  \n",
    "AdaBoostClassifier: 0.5565986975931697  \n",
    "RandomForest: 0.5139748064829897  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded results for models2+weighted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 test scores:  \n",
    "LogisticRegression: 0.505849582172702  \n",
    "AdaBoostClassifier: 0.5042492917847025  \n",
    "GaussianNB: 0.48763657274295574  \n",
    "BernoulliNB: 0.44458052663808934  \n",
    "RandomForest: 0.319634703196347  \n",
    "  \n",
    "AUROC test scores:  \n",
    "LogisticRegression: 0.5771670300715352  \n",
    "AdaBoostClassifier: 0.5767470336446392  \n",
    "GaussianNB: 0.5674402471501362  \n",
    "BernoulliNB: 0.5494587583359365  \n",
    "RandomForest: 0.5154155404946555  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded results for models+concatenation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 test scores:  \n",
    "AdaBoostClassifier: 0.5018607123870281  \n",
    "LogisticRegression: 0.4999999999999999  \n",
    "GaussianNB: 0.49914529914529915  \n",
    "BernoulliNB: 0.4936268829663963  \n",
    "RandomForest: 0.31145038167938927  \n",
    "  \n",
    "AUROC test scores:  \n",
    "LogisticRegression: 0.5795255920957091  \n",
    "BernoulliNB: 0.5757915940118121  \n",
    "GaussianNB: 0.5708138005393087  \n",
    "AdaBoostClassifier: 0.5635594592702718  \n",
    "RandomForest: 0.5182442477728174  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best method is unweighted averaging, so we will use this in combination with the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GaussianNB.\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.6s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for BernoulliNB.\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.5s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForest.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   24.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 test scores:\n",
      "AdaBoostClassifier: 0.5870563674321504\n",
      "GaussianNB: 0.5662953647143371\n",
      "BernoulliNB: 0.5430656934306569\n",
      "LogisticRegression: 0.3202911737943585\n",
      "RandomForest: 0.30173124484748554\n",
      "\n",
      "AUROC test scores:\n",
      "AdaBoostClassifier: 0.6368942831381882\n",
      "LogisticRegression: 0.6274642141104173\n",
      "BernoulliNB: 0.5802370785800778\n",
      "RandomForest: 0.5357806657465749\n",
      "GaussianNB: 0.49891083016159415\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate((embs_train_unweighted, train_data), axis=1)\n",
    "X_valid = np.concatenate((embs_valid_unweighted, valid_data), axis=1)\n",
    "X_test = np.concatenate((embs_test_unweighted, test_data), axis=1)\n",
    "\n",
    "X_train = np.concatenate((X_train, X_valid))\n",
    "y_train = np.concatenate((train_y, valid_y))\n",
    "y_test = test_y\n",
    "\n",
    "summary = evaluate(X_train, y_train, X_test, y_test, models, params)\n",
    "# print(summary[['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard coded results for models+weighted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 test scores:  \n",
    "AdaBoostClassifier: 0.5867112411199331  \n",
    "GaussianNB: 0.5656855707106964  \n",
    "BernoulliNB: 0.5355566454144188  \n",
    "LogisticRegression: 0.31444241316270566  \n",
    "RandomForest: 0.305532617671346  \n",
    "  \n",
    "AUROC test scores:  \n",
    "AdaBoostClassifier: 0.637052042990082  \n",
    "LogisticRegression: 0.6276366007035462  \n",
    "BernoulliNB: 0.5803964055828181  \n",
    "RandomForest: 0.5330961363463028  \n",
    "GaussianNB: 0.497675915294452  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions for USE\n",
    "\n",
    "- We have seen using only text features has some predictive power, but not enough to improve the score achieved with only non-textual data\n",
    "- Weighted method decreases f1 score but mantains auroc. Perhaps the used values for the weights are not optimal\n",
    "- Something similar occurs when using concatenation method\n",
    "- Standardization+feature selection was not useful. The best models from the grid used all the features and others using less features performed similarly \n",
    "- Using the more powerful version of USE didn't make significant improvements (slightly decreased std)\n",
    "- Words are very specific to medical domain, fine tuning might offer significant improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
