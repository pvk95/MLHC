{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('exercise_data/human_dna_train_split.csv')\n",
    "df_valid = pd.read_csv('exercise_data/human_dna_validation_split.csv')\n",
    "df_test  = pd.read_csv('exercise_data/human_dna_test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training_set: (500000, 2)\n",
      "Shape of test set: (33333, 2)\n",
      "Shape of validation set: (33333, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of training_set: {df_train.shape}\")\n",
    "print(f\"Shape of test set: {df_test.shape}\")\n",
    "print(f\"Shape of validation set: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, CondensedNearestNeighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_dataframe(df):\n",
    "    X = np.vstack(df['sequences'].values)\n",
    "    y = df['labels'].values\n",
    "    undersampler = RandomUnderSampler()\n",
    "    X_undersampled, y_undersampled = undersampler.fit_resample(X, y)\n",
    "    undersampled_df = pd.DataFrame({'labels':y_undersampled, 'sequences': X_undersampled[:,0]})\n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before undersampling 500000\n",
      "Size after undersampling 2942\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size before undersampling {df_train.shape[0]}\")\n",
    "df_train_undersampled = undersample_dataframe(df_train)\n",
    "print(f\"Size after undersampling {df_train_undersampled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping DNA to a vector\n",
    "\n",
    "We will map the DNA into a vector, by mapping each Character (A,T,C,G) into a one-hot vector and then concatonating all these vectors together. As we have a string of 398 Characters this gives us a final vector of length 1592. \n",
    "\n",
    "*This operation takes some time, please be patient*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility\n",
    "df_train_undersampled['sequences'] = df_train_undersampled['sequences'].map(utility.map_dna_into_vector)\n",
    "df_valid['sequences'] = df_valid['sequences'].map(utility.map_dna_into_vector)\n",
    "df_test['sequences']  = df_test['sequences'].map(utility.map_dna_into_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame for later Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(data=[], columns=[\"Name\", \"AUROC\", \"AUPRC\", \"f1_cv\", \"f1_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought that the evaluation of the models was kind of slow so we created a parallel job. Each process evaluates a model using the evaluator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(args):\n",
    "    # read out parameters\n",
    "    model, params, train, valid = args\n",
    "    \n",
    "    # get train and validation data\n",
    "    train_data = np.vstack(train['sequences'].values)\n",
    "    valid_data = np.vstack(valid['sequences'].values)\n",
    "    \n",
    "    # Fit Model with parameters \n",
    "    m = model(**params)\n",
    "    m = m.fit(train_data, train['labels'].values)\n",
    "    \n",
    "    # Get prediction of our data\n",
    "    pred_val = m.predict(valid_data)\n",
    "    score = f1_score(valid['labels'].values, pred_val)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we evaluate all models with all the different parameters we specified. First we evaluate the model using the training data and the evaluation data. Then we train the best of those models on the combined training and validation data, to get an estimate of the f1-score on the test-data. \n",
    "\n",
    "*It's important to note that we evaluate all the models based on their performance on the validation-set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, params, train, valid, test, eval_df):\n",
    "    # Put Data into a usable Matrix Format\n",
    "    train_data = np.vstack(train['sequences'].values)\n",
    "    valid_data = np.vstack(valid['sequences'].values)\n",
    "    test_data = np.vstack(valid['sequences'].values)\n",
    "    \n",
    "    combined_data = np.vstack([train_data, valid_data])\n",
    "    combined_labels = np.hstack([train['labels'].values, valid['labels'].values])\n",
    "    \n",
    "    # Create Instance of the Model\n",
    "    m = model()\n",
    "    \n",
    "    \n",
    "    # Search for the best params in our model and print the best score\n",
    "    p = Pool(cpu_count())\n",
    "    grid = ParameterGrid(params)\n",
    "    scores = p.map(evaluator, [(model, params, train, valid) for params in grid])\n",
    "    p.close()\n",
    "    \n",
    "    # Get best parameters and scores\n",
    "    best_score = np.max(scores)\n",
    "    best_params = grid[np.argmax(scores)]\n",
    "    print(f\"The best score was {best_score}\")\n",
    "    \n",
    "    \n",
    "    # Train our best model on the whole train-dataset\n",
    "    best_estimator = model(**best_params)\n",
    "    best_estimotor = best_estimator.fit(combined_data, combined_labels)\n",
    "    \n",
    "    # Evaluate on the Test set\n",
    "    pred_val = best_estimator.predict(test_data)\n",
    "    true_val = test['labels'].values\n",
    "    auroc, auprc, f1 = utility.get_scores(true_val, pred_val)\n",
    "    \n",
    "    # Append to our Dataframe\n",
    "    eval_df = eval_df.append({'Name': model.__name__, 'AUROC':auroc , 'AUPRC': auprc, 'f1_cv':best_score, \"f1_test\": f1}, ignore_index=True)\n",
    "    return (best_estimator, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We test logistic Regression with a few different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1, 10, 100],\n",
    "    'class_weight': ['balanced'],\n",
    "    'solver': ['liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was 0.022753889911326753\n"
     ]
    }
   ],
   "source": [
    "lg_best_estimator, eval_df = evaluate_model(LogisticRegression, params, df_train_undersampled, df_valid, df_test, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC\n",
    "\n",
    "We test Support-Vector-Classifer with different parameters. Be careful, this task takes a long time and uses a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'kernel': ['linear', 'rbf', 'poly'],\n",
    "          'C': [1, 10],\n",
    "          'class_weight': ['balanced'],\n",
    "          'gamma': ['auto', 'scale'],\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was 0.035142348754448396\n"
     ]
    }
   ],
   "source": [
    "svc_best_estimator, eval_df = evaluate_model(SVC, params, df_train_undersampled, df_valid, df_test, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "We test the RAndom-Forest C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators':[10, 100, 300],    \n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was 0.03183791606367583\n"
     ]
    }
   ],
   "source": [
    "rfc_best_estimator, eval_df = evaluate_model(RandomForestClassifier, params, df_train_undersampled, df_valid, df_test, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gaussian Process Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip this classifier, as the memory-requirements crashes the kernel with ~64gb ram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
