{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('exercise_data/human_dna_train_split.csv')\n",
    "df_valid = pd.read_csv('exercise_data/human_dna_validation_split.csv')\n",
    "df_test  = pd.read_csv('exercise_data/human_dna_test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of training_set: {df_train.shape}\")\n",
    "print(f\"Shape of test set: {df_test.shape}\")\n",
    "print(f\"Shape of validation set: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, CondensedNearestNeighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_dataframe(df):\n",
    "    X = np.vstack(df['sequences'].values)\n",
    "    y = df['labels'].values\n",
    "    undersampler = RandomUnderSampler()\n",
    "    X_undersampled, y_undersampled = undersampler.fit_resample(X, y)\n",
    "    undersampled_df = pd.DataFrame({'labels':y_undersampled, 'sequences': X_undersampled[:,0]})\n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size before undersampling {df_train.shape[0]}\")\n",
    "df_train_undersampled = undersample_dataframe(df_train)\n",
    "print(f\"Size after undersampling {df_train_undersampled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping DNA to a vector\n",
    "\n",
    "We will map the DNA into a vector, by mapping each Character (A,T,C,G) into a one-hot vector and then concatonating all these vectors together. As we have a string of 398 Characters this gives us a final vector of length 1592. \n",
    "\n",
    "*This operation takes some time, please be patient*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility\n",
    "df_train_undersampled['sequences'] = df_train_undersampled['sequences'].map(utility.map_dna_into_vector)\n",
    "df_valid['sequences'] = df_valid['sequences'].map(utility.map_dna_into_vector)\n",
    "df_test['sequences']  = df_test['sequences'].map(utility.map_dna_into_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "At first it seemed like to be a good idea to compress the Vector of length 1592 to the length of 200. Which seemed to be reasonable given to plot PCA produces. It did help somewhat with the training-time, but the f1-score got way worse <0.08. So we dropped the idea of using PCA for compressing the trainings-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca = pca.fit(np.vstack(df_train_undersampled['sequences'].values))\n",
    "\n",
    "plt.plot(range(len(pca.explained_variance_)), pca.explained_variance_ratio_, )\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Index of Principal Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame for later Evalution\n",
    "\n",
    "We create the dataframe eval_df for the final evalution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(data=[], columns=[\"Name\", \"AUROC\", \"AUPRC\", \"f1_cv\", \"f1_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought that the evaluation of the models was kind of slow so we created a parallel job. Each process evaluates a model using the evaluator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(args):\n",
    "    # read out parameters\n",
    "    model, params, train, valid = args\n",
    "    \n",
    "    # get train and validation data\n",
    "    train_data = np.vstack(train['sequences'].values)\n",
    "    valid_data = np.vstack(valid['sequences'].values)\n",
    "    \n",
    "    # Fit Model with parameters \n",
    "    m = model(**params)\n",
    "    m = m.fit(train_data, train['labels'].values)\n",
    "    \n",
    "    # Get prediction of our data\n",
    "    pred_val = m.predict(valid_data)\n",
    "    score = f1_score(valid['labels'].values, pred_val)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we evaluate all models with all the different parameters we specified. First we evaluate the model using the training data and the evaluation data. Then we train the best of those models on the combined training and validation data, to get an estimate of the f1-score on the test-data. \n",
    "\n",
    "*It's important to note that we evaluate all the models based on their performance on the validation-set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, params, train, valid, test, eval_df):\n",
    "    # Put Data into a usable Matrix Format\n",
    "    train_data = np.vstack(train['sequences'].values)\n",
    "    valid_data = np.vstack(valid['sequences'].values)\n",
    "    test_data = np.vstack(valid['sequences'].values)\n",
    "    \n",
    "    combined_data = np.vstack([train_data, valid_data])\n",
    "    combined_labels = np.hstack([train['labels'].values, valid['labels'].values])\n",
    "    \n",
    "    # Create Instance of the Model\n",
    "    m = model()\n",
    "    \n",
    "    \n",
    "    # Search for the best params in our model and print the best score\n",
    "    p = Pool(cpu_count())\n",
    "    grid = ParameterGrid(params)\n",
    "    scores = p.map(evaluator, [(model, params, train, valid) for params in grid])\n",
    "    p.close()\n",
    "    \n",
    "    # Get best parameters and scores\n",
    "    best_score = np.max(scores)\n",
    "    best_params = grid[np.argmax(scores)]\n",
    "    print(f\"The best score was {best_score}\")\n",
    "    \n",
    "    \n",
    "    # Train our best model on the whole train-dataset\n",
    "    best_estimator = model(**best_params)\n",
    "    best_estimotor = best_estimator.fit(combined_data, combined_labels)\n",
    "    \n",
    "    # Evaluate on the Test set\n",
    "    pred_val = best_estimator.predict(test_data)\n",
    "    true_val = test['labels'].values\n",
    "    auroc, auprc, f1 = utility.get_scores(true_val, pred_val)\n",
    "    \n",
    "    # Append to our Dataframe\n",
    "    eval_df = eval_df.append({'Name': model.__name__, 'AUROC':auroc , 'AUPRC': auprc, 'f1_cv':best_score, \"f1_test\": f1}, ignore_index=True)\n",
    "    return (best_estimator, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We test logistic Regression with a few different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1, 10, 100],\n",
    "    'class_weight': ['balanced'],\n",
    "    'solver': ['liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_best_estimator, eval_df = evaluate_model(LogisticRegression, params, df_train_undersampled, df_valid, df_test, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC\n",
    "\n",
    "We test Support-Vector-Classifer with different parameters. Be careful, this task takes a long time and uses a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'kernel': ['linear', 'rbf', 'poly'],\n",
    "          'C': [1, 10],\n",
    "          'class_weight': ['balanced'],\n",
    "          'gamma': ['auto', 'scale'],\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best_estimator, eval_df = evaluate_model(SVC, params, df_train_undersampled, df_valid, df_test, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "We test the RAndom-Forest C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators':[10, 100, 300],    \n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_best_estimator, eval_df = evaluate_model(RandomForestClassifier, params, df_train_undersampled, df_valid, df_test, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gaussian Process Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip this classifier, as the memory-requirements crashes the kernel with ~64gb ram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
