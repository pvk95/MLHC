{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTB Diagnostic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import models\n",
    "import types\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.mixture\n",
    "from sklearn.metrics import roc_curve,precision_recall_curve,auc,accuracy_score,f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for getting scores and visualization\n",
    "def getScores(model_name,Y_test,pred_test,metrics_df):\n",
    "    fpr, tpr, _ = roc_curve(Y_test, pred_test)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    precision, recall, _ = precision_recall_curve(Y_test, pred_test)\n",
    "    auprc = auc(recall, precision)\n",
    "    pred_test = (pred_test > 0.5).astype(np.int8)\n",
    "    f1 = f1_score(Y_test, pred_test)\n",
    "    acc = accuracy_score(Y_test, pred_test)\n",
    "    curr_metrics = {'Name': model_name, 'f1_score': f1, \"AUROC\": auroc, \"AUPRC\": auprc, \"ACC\": acc}\n",
    "    metrics_df = metrics_df.append(curr_metrics, ignore_index=True)\n",
    "    return metrics_df\n",
    "\n",
    "def visualize(df,title):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    np.random.seed(0)\n",
    "    n_sub_plots = 5\n",
    "    for i in range(n_sub_plots):\n",
    "        plt.subplot(n_sub_plots, 1, i + 1)\n",
    "        plt.plot(df.iloc[np.random.choice(len(df[1])), :])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#Set the gpu number you want to run on\n",
    "#Caution: Setting to a number not recognizable will not run on GPU\n",
    "gpu = 7 #gpu = 0\n",
    "lstm_out = 100\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "df_1 = pd.read_csv(\"exercise_data/heartbeat/ptbdb_normal.csv\", header=None)\n",
    "df_2 = pd.read_csv(\"exercise_data/heartbeat/ptbdb_abnormal.csv\", header=None)\n",
    "\n",
    "visualize(df_1,'Normal EEG')\n",
    "visualize(df_2,'Abnormal EEG')\n",
    "\n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(186))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(186))].values)[..., np.newaxis]\n",
    "\n",
    "#X_ft = standard.getfeatures(X)\n",
    "#X_test_ft = standard.getfeatures(X_test)\n",
    "\n",
    "metrics_df = pd.DataFrame(data=[],columns=['Name','f1_score','AUROC','AUPRC','ACC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Neural network models and Random forests compared to baseline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ = [\n",
    "    sklearn.mixture.GaussianMixture(n_components=2),\n",
    "    sklearn.mixture.BayesianGaussianMixture(n_components=2),\n",
    "    models.LSTM_Model(),\n",
    "    RandomForestClassifier(n_jobs=-1),\n",
    "    models.Residual_CNN(),\n",
    "    models.CNN_Model(),\n",
    "]\n",
    "\n",
    "params = [\n",
    "    # Gaussian mixture\n",
    "    {\n",
    "\n",
    "    },\n",
    "    # Bayesian Mixture\n",
    "    {\n",
    "\n",
    "    },\n",
    "    # LSTM\n",
    "    {\n",
    "        'verbose': [0],\n",
    "        'hidden': [16, 32, 64],\n",
    "        'dense': [16, 32, 64]\n",
    "    },\n",
    "    # RandomForestClassifier\n",
    "    {\n",
    "        'n_estimators' : [10, 100, 200],\n",
    "        'n_jobs':  [-1]\n",
    "    },\n",
    "    # Residual_CNN\n",
    "    {\n",
    "        'deepness': range(1,6),\n",
    "        'verbose': [0]\n",
    "    },\n",
    "    # CNN_Model\n",
    "    {   \n",
    "        'conv1_size': [16, 32],\n",
    "        'conv2_size': [32, 64],\n",
    "        'conv3_size': [128, 256],\n",
    "        'dense_size': [16, 32, 64],\n",
    "        'verbose': [0]\n",
    "    },\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models and report the metric scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = []\n",
    "for param, model in zip(params, models_):\n",
    "    clf = RandomizedSearchCV(model, param, cv=2, n_iter=5, verbose=2)\n",
    "    if type(model) == RandomForestClassifier or \\\n",
    "        type(model) == sklearn.mixture.GaussianMixture or \\\n",
    "        type(model) == sklearn.mixture.BayesianGaussianMixture:\n",
    "        clf.fit(np.squeeze(X), Y)\n",
    "        model = clf.best_estimator_\n",
    "        model.getScores = types.MethodType(models.CNN_Model.getScores, model)\n",
    "        _,metrics_df = model.getScores(np.squeeze(X_test), Y_test, metrics_df)\n",
    "    else:\n",
    "        clf.fit(X, Y)\n",
    "        model = clf.best_estimator_\n",
    "        pred,metrics_df = model.getScores(X_test, Y_test, metrics_df)\n",
    "        model_preds.append(pred)\n",
    "        \n",
    "    print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = np.array(model_preds)\n",
    "model_preds = np.squeeze(model_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg ensemble:\n",
    "avg_pred = np.mean(model_preds,axis=0)\n",
    "metrics_df = getScores('Ensemble(Avg)',Y_test=Y_test,pred_test=avg_pred,metrics_df=metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression(n_jobs=-1)\n",
    "X_lg = np.transpose(model_preds,[1,0])\n",
    "lg.fit(X_lg,Y_test)\n",
    "lg_pred = lg.predict_proba(X_lg)[:,1]\n",
    "metrics_df = getScores('Ensemble(LG)',Y_test=Y_test,pred_test=lg_pred,metrics_df=metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
