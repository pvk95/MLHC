{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MITBIH DATA\n",
    "\n",
    "We took the first 186 values as input (instead of the first 187) because the PTBDB uses only 186 values as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitbih_train = pd.read_csv('exercise_data/heartbeat/mitbih_train.csv', header=None)\n",
    "\n",
    "X = mitbih_train[list(range(186))].values[..., np.newaxis]\n",
    "Y = mitbih_train[187]\n",
    "X, Y = shuffle(X, Y, random_state=42)\n",
    "Y_binary = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model on MITBIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/martin/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 78798 samples, validate on 8756 samples\n",
      "WARNING:tensorflow:From /home/martin/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8975\n",
      "Epoch 00001: val_acc improved from -inf to 0.94164, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 24s 308us/sample - loss: 0.3912 - acc: 0.8975 - val_loss: 0.2244 - val_acc: 0.9416\n",
      "Epoch 2/1000\n",
      "78624/78798 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9450\n",
      "Epoch 00002: val_acc improved from 0.94164 to 0.95683, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 23s 288us/sample - loss: 0.2043 - acc: 0.9450 - val_loss: 0.1596 - val_acc: 0.9568\n",
      "Epoch 3/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9544\n",
      "Epoch 00003: val_acc improved from 0.95683 to 0.96071, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 23s 287us/sample - loss: 0.1599 - acc: 0.9545 - val_loss: 0.1445 - val_acc: 0.9607\n",
      "Epoch 4/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9546\n",
      "Epoch 00004: val_acc did not improve from 0.96071\n",
      "78798/78798 [==============================] - 23s 287us/sample - loss: 0.1605 - acc: 0.9546 - val_loss: 0.1570 - val_acc: 0.9556\n",
      "Epoch 5/1000\n",
      "78752/78798 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9611\n",
      "Epoch 00005: val_acc improved from 0.96071 to 0.96562, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 23s 292us/sample - loss: 0.1360 - acc: 0.9611 - val_loss: 0.1209 - val_acc: 0.9656\n",
      "Epoch 6/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9659\n",
      "Epoch 00006: val_acc improved from 0.96562 to 0.96768, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 23s 291us/sample - loss: 0.1211 - acc: 0.9659 - val_loss: 0.1080 - val_acc: 0.9677\n",
      "Epoch 7/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9703\n",
      "Epoch 00007: val_acc improved from 0.96768 to 0.97042, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 23s 288us/sample - loss: 0.1033 - acc: 0.9703 - val_loss: 0.1000 - val_acc: 0.9704\n",
      "Epoch 8/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9718\n",
      "Epoch 00008: val_acc improved from 0.97042 to 0.97487, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 23s 288us/sample - loss: 0.0963 - acc: 0.9718 - val_loss: 0.0931 - val_acc: 0.9749\n",
      "Epoch 9/1000\n",
      "78752/78798 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9739\n",
      "Epoch 00009: val_acc did not improve from 0.97487\n",
      "78798/78798 [==============================] - 23s 291us/sample - loss: 0.0897 - acc: 0.9739 - val_loss: 0.0905 - val_acc: 0.9749\n",
      "Epoch 10/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9752\n",
      "Epoch 00010: val_acc improved from 0.97487 to 0.97933, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 23s 291us/sample - loss: 0.0841 - acc: 0.9752 - val_loss: 0.0795 - val_acc: 0.9793\n",
      "Epoch 11/1000\n",
      "78656/78798 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9758\n",
      "Epoch 00011: val_acc did not improve from 0.97933\n",
      "78798/78798 [==============================] - 23s 291us/sample - loss: 0.0808 - acc: 0.9758 - val_loss: 0.0834 - val_acc: 0.9785\n",
      "Epoch 12/1000\n",
      "78752/78798 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9774\n",
      "Epoch 00012: val_acc did not improve from 0.97933\n",
      "78798/78798 [==============================] - 23s 290us/sample - loss: 0.0759 - acc: 0.9774 - val_loss: 0.0784 - val_acc: 0.9792\n",
      "Epoch 13/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9780\n",
      "Epoch 00013: val_acc did not improve from 0.97933\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "78798/78798 [==============================] - 23s 290us/sample - loss: 0.0729 - acc: 0.9780 - val_loss: 0.0864 - val_acc: 0.9762\n",
      "Epoch 14/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9833\n",
      "Epoch 00014: val_acc improved from 0.97933 to 0.98230, saving model to LSTM_Model.h5\n",
      "78798/78798 [==============================] - 23s 291us/sample - loss: 0.0553 - acc: 0.9833 - val_loss: 0.0695 - val_acc: 0.9823\n",
      "Epoch 15/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9841\n",
      "Epoch 00015: val_acc did not improve from 0.98230\n",
      "78798/78798 [==============================] - 23s 291us/sample - loss: 0.0523 - acc: 0.9841 - val_loss: 0.0694 - val_acc: 0.9815\n",
      "Epoch 16/1000\n",
      "78752/78798 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9845\n",
      "Epoch 00016: val_acc did not improve from 0.98230\n",
      "78798/78798 [==============================] - 23s 291us/sample - loss: 0.0509 - acc: 0.9845 - val_loss: 0.0692 - val_acc: 0.9816\n",
      "Epoch 17/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9849\n",
      "Epoch 00017: val_acc did not improve from 0.98230\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "78798/78798 [==============================] - 23s 292us/sample - loss: 0.0499 - acc: 0.9849 - val_loss: 0.0696 - val_acc: 0.9816\n",
      "Epoch 18/1000\n",
      "78720/78798 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9856\n",
      "Epoch 00018: val_acc did not improve from 0.98230\n",
      "78798/78798 [==============================] - 23s 293us/sample - loss: 0.0474 - acc: 0.9856 - val_loss: 0.0685 - val_acc: 0.9820\n",
      "Epoch 19/1000\n",
      "78752/78798 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9855\n",
      "Epoch 00019: val_acc did not improve from 0.98230\n",
      "78798/78798 [==============================] - 23s 292us/sample - loss: 0.0470 - acc: 0.9856 - val_loss: 0.0683 - val_acc: 0.9821\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "rnn_model = models.LSTM_Model(outputs=5)\n",
    "rnn_model.fit(X, Y_binary)\n",
    "rnn_model.model.save('transfer_learning.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PTBDB DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_1 = pd.read_csv(\"exercise_data/heartbeat/ptbdb_normal.csv\", header=None)\n",
    "ptbdb_2 = pd.read_csv(\"exercise_data/heartbeat/ptbdb_abnormal.csv\", header=None)\n",
    "ptbdb = pd.concat([ptbdb_1, ptbdb_2])\n",
    "X = ptbdb[list(range(186))].values[..., np.newaxis]\n",
    "Y = ptbdb[187]\n",
    "X, Y = shuffle(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " train method for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train method for later use\n",
    "def train(model, X, Y):\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "    redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=1)\n",
    "    callbacks_list = [early, redonplat]  # early\n",
    "\n",
    "    model.fit(X, Y, epochs=1000, verbose=1, callbacks=callbacks_list, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze Everything expect the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 186, 1)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                8960      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "ouput (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 17,345\n",
      "Trainable params: 65\n",
      "Non-trainable params: 17,280\n",
      "_________________________________________________________________\n",
      "Train on 13096 samples, validate on 1456 samples\n",
      "Epoch 1/1000\n",
      "13096/13096 [==============================] - 2s 178us/sample - loss: 1.8291 - acc: 0.6384 - val_loss: 1.1517 - val_acc: 0.7040\n",
      "Epoch 2/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 1.1305 - acc: 0.6719 - val_loss: 0.9934 - val_acc: 0.6944\n",
      "Epoch 3/1000\n",
      "13096/13096 [==============================] - 2s 165us/sample - loss: 0.9872 - acc: 0.6899 - val_loss: 0.9003 - val_acc: 0.7047\n",
      "Epoch 4/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 0.9048 - acc: 0.6951 - val_loss: 0.7482 - val_acc: 0.7287\n",
      "Epoch 5/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 0.8687 - acc: 0.7103 - val_loss: 0.7502 - val_acc: 0.7301\n",
      "Epoch 6/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 0.9354 - acc: 0.7017 - val_loss: 2.9330 - val_acc: 0.5996\n",
      "Epoch 7/1000\n",
      "13096/13096 [==============================] - 2s 163us/sample - loss: 0.8997 - acc: 0.7099 - val_loss: 0.7183 - val_acc: 0.7383\n",
      "Epoch 8/1000\n",
      "13096/13096 [==============================] - 2s 164us/sample - loss: 0.7405 - acc: 0.7239 - val_loss: 0.6721 - val_acc: 0.7431\n",
      "Epoch 9/1000\n",
      "13096/13096 [==============================] - 2s 163us/sample - loss: 0.6711 - acc: 0.7279 - val_loss: 0.6498 - val_acc: 0.7473\n",
      "Epoch 10/1000\n",
      "13096/13096 [==============================] - 2s 165us/sample - loss: 0.6629 - acc: 0.7316 - val_loss: 0.6308 - val_acc: 0.7479\n",
      "Epoch 11/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 0.6740 - acc: 0.7289 - val_loss: 0.6056 - val_acc: 0.7507\n",
      "Epoch 12/1000\n",
      "13096/13096 [==============================] - 2s 163us/sample - loss: 0.6197 - acc: 0.7397 - val_loss: 0.6113 - val_acc: 0.7603\n",
      "Epoch 13/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 0.6154 - acc: 0.7419 - val_loss: 0.5891 - val_acc: 0.7679\n",
      "Epoch 14/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 0.6205 - acc: 0.7421 - val_loss: 0.5804 - val_acc: 0.7603\n",
      "Epoch 15/1000\n",
      "13096/13096 [==============================] - 2s 163us/sample - loss: 0.5823 - acc: 0.7479 - val_loss: 0.5780 - val_acc: 0.7692\n",
      "Epoch 16/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 0.5748 - acc: 0.7573 - val_loss: 0.5751 - val_acc: 0.7734\n",
      "Epoch 17/1000\n",
      "13096/13096 [==============================] - 2s 161us/sample - loss: 0.5612 - acc: 0.7608 - val_loss: 0.5570 - val_acc: 0.7788\n",
      "Epoch 18/1000\n",
      "13096/13096 [==============================] - 2s 160us/sample - loss: 1.2032 - acc: 0.7188 - val_loss: 3.0999 - val_acc: 0.7349\n",
      "Epoch 19/1000\n",
      "13096/13096 [==============================] - 2s 161us/sample - loss: 1.3402 - acc: 0.7179 - val_loss: 0.7794 - val_acc: 0.7356\n",
      "Epoch 20/1000\n",
      "12960/13096 [============================>.] - ETA: 0s - loss: 0.6624 - acc: 0.7096\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "13096/13096 [==============================] - 2s 168us/sample - loss: 0.6640 - acc: 0.7092 - val_loss: 0.5972 - val_acc: 0.7198\n",
      "Epoch 21/1000\n",
      "13096/13096 [==============================] - 2s 161us/sample - loss: 0.6041 - acc: 0.7160 - val_loss: 0.5815 - val_acc: 0.7212\n",
      "Epoch 22/1000\n",
      "13096/13096 [==============================] - 2s 162us/sample - loss: 0.5979 - acc: 0.7164 - val_loss: 0.5761 - val_acc: 0.7225\n",
      "Epoch 00022: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Freeze everything except the last layer and train\n",
    "model = load_model('transfer_learning.h5')\n",
    "out = Dense(1, name='ouput')(model.layers[-2].output)\n",
    "model2 = Model(model.input, out)\n",
    "\n",
    "for x in range(4):\n",
    "    model2.layers[x].trainable = False\n",
    "\n",
    "model2.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "model2.summary()\n",
    "train(model2, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't freeze any layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 186, 1)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                8960      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "ouput (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 17,345\n",
      "Trainable params: 17,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13096 samples, validate on 1456 samples\n",
      "Epoch 1/1000\n",
      "13096/13096 [==============================] - 4s 338us/sample - loss: 0.9726 - acc: 0.7979 - val_loss: 0.4555 - val_acc: 0.8585\n",
      "Epoch 2/1000\n",
      "13096/13096 [==============================] - 4s 296us/sample - loss: 0.4799 - acc: 0.8560 - val_loss: 0.4868 - val_acc: 0.8242\n",
      "Epoch 3/1000\n",
      "13096/13096 [==============================] - 4s 295us/sample - loss: 0.4530 - acc: 0.8664 - val_loss: 0.3793 - val_acc: 0.8949\n",
      "Epoch 4/1000\n",
      "13096/13096 [==============================] - 4s 295us/sample - loss: 0.6320 - acc: 0.7895 - val_loss: 0.4861 - val_acc: 0.8304\n",
      "Epoch 5/1000\n",
      "13096/13096 [==============================] - 4s 296us/sample - loss: 0.6454 - acc: 0.8169 - val_loss: 0.4527 - val_acc: 0.8530\n",
      "Epoch 6/1000\n",
      "13088/13096 [============================>.] - ETA: 0s - loss: 0.4704 - acc: 0.8321\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "13096/13096 [==============================] - 4s 318us/sample - loss: 0.4702 - acc: 0.8322 - val_loss: 0.4211 - val_acc: 0.8743\n",
      "Epoch 7/1000\n",
      "13096/13096 [==============================] - 4s 297us/sample - loss: 0.4116 - acc: 0.8781 - val_loss: 0.4054 - val_acc: 0.8757\n",
      "Epoch 8/1000\n",
      "13096/13096 [==============================] - 4s 299us/sample - loss: 0.4070 - acc: 0.8820 - val_loss: 0.4072 - val_acc: 0.8805\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = load_model('transfer_learning.h5')\n",
    "out = Dense(1, name='ouput')(model.layers[-2].output)\n",
    "model2 = Model(model.input, out)\n",
    "model2.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "model2.summary()\n",
    "train(model2, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Freeze - then unfreeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 186, 1)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                8960      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "ouput (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 17,345\n",
      "Trainable params: 65\n",
      "Non-trainable params: 17,280\n",
      "_________________________________________________________________\n",
      "Train on 13096 samples, validate on 1456 samples\n",
      "Epoch 1/1000\n",
      "13096/13096 [==============================] - 3s 210us/sample - loss: 3.8688 - acc: 0.6273 - val_loss: 2.5268 - val_acc: 0.6422\n",
      "Epoch 2/1000\n",
      "13096/13096 [==============================] - 2s 164us/sample - loss: 2.5950 - acc: 0.6735 - val_loss: 2.1153 - val_acc: 0.6669\n",
      "Epoch 3/1000\n",
      "13096/13096 [==============================] - 2s 170us/sample - loss: 2.0424 - acc: 0.6810 - val_loss: 1.9984 - val_acc: 0.6930\n",
      "Epoch 4/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 1.9493 - acc: 0.6970 - val_loss: 1.9499 - val_acc: 0.7047\n",
      "Epoch 5/1000\n",
      "13096/13096 [==============================] - 2s 165us/sample - loss: 1.8591 - acc: 0.7067 - val_loss: 1.8518 - val_acc: 0.7102\n",
      "Epoch 6/1000\n",
      "13096/13096 [==============================] - 2s 165us/sample - loss: 1.7816 - acc: 0.7148 - val_loss: 1.8548 - val_acc: 0.7218\n",
      "Epoch 7/1000\n",
      "13096/13096 [==============================] - 2s 166us/sample - loss: 1.8133 - acc: 0.7163 - val_loss: 1.7495 - val_acc: 0.7225\n",
      "Epoch 8/1000\n",
      "13096/13096 [==============================] - 2s 168us/sample - loss: 1.5856 - acc: 0.7221 - val_loss: 1.4762 - val_acc: 0.7246\n",
      "Epoch 9/1000\n",
      "13096/13096 [==============================] - 2s 165us/sample - loss: 1.5907 - acc: 0.7091 - val_loss: 1.3517 - val_acc: 0.7067\n",
      "Epoch 10/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 1.3517 - acc: 0.7165 - val_loss: 1.2963 - val_acc: 0.7239\n",
      "Epoch 11/1000\n",
      "13096/13096 [==============================] - 2s 166us/sample - loss: 1.2472 - acc: 0.7233 - val_loss: 1.1926 - val_acc: 0.7301\n",
      "Epoch 12/1000\n",
      "13096/13096 [==============================] - 2s 166us/sample - loss: 1.1689 - acc: 0.7311 - val_loss: 1.1482 - val_acc: 0.7335\n",
      "Epoch 13/1000\n",
      "13096/13096 [==============================] - 2s 166us/sample - loss: 1.0173 - acc: 0.7227 - val_loss: 1.0337 - val_acc: 0.7081\n",
      "Epoch 14/1000\n",
      "13096/13096 [==============================] - 2s 166us/sample - loss: 1.6046 - acc: 0.7378 - val_loss: 0.8488 - val_acc: 0.7260\n",
      "Epoch 15/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.8570 - acc: 0.7411 - val_loss: 0.8383 - val_acc: 0.7424\n",
      "Epoch 16/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.8160 - acc: 0.7508 - val_loss: 0.7767 - val_acc: 0.7555\n",
      "Epoch 17/1000\n",
      "13096/13096 [==============================] - 2s 168us/sample - loss: 0.7620 - acc: 0.7561 - val_loss: 0.7749 - val_acc: 0.7486\n",
      "Epoch 18/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.7287 - acc: 0.7623 - val_loss: 0.7630 - val_acc: 0.7582\n",
      "Epoch 19/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.6650 - acc: 0.7645 - val_loss: 0.6551 - val_acc: 0.7788\n",
      "Epoch 20/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.6343 - acc: 0.7721 - val_loss: 0.6474 - val_acc: 0.7782\n",
      "Epoch 21/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.6167 - acc: 0.7766 - val_loss: 0.6241 - val_acc: 0.7850\n",
      "Epoch 22/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.6308 - acc: 0.7805 - val_loss: 0.6269 - val_acc: 0.7795\n",
      "Epoch 23/1000\n",
      "13096/13096 [==============================] - 2s 166us/sample - loss: 0.5851 - acc: 0.7784 - val_loss: 0.5842 - val_acc: 0.7830\n",
      "Epoch 24/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.5608 - acc: 0.7847 - val_loss: 0.5818 - val_acc: 0.7864\n",
      "Epoch 25/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.5420 - acc: 0.7849 - val_loss: 0.5651 - val_acc: 0.7946\n",
      "Epoch 26/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.5243 - acc: 0.7940 - val_loss: 0.5966 - val_acc: 0.7981\n",
      "Epoch 27/1000\n",
      "13096/13096 [==============================] - 2s 170us/sample - loss: 0.5096 - acc: 0.7857 - val_loss: 0.5424 - val_acc: 0.7940\n",
      "Epoch 28/1000\n",
      "13096/13096 [==============================] - 2s 170us/sample - loss: 0.4657 - acc: 0.7905 - val_loss: 0.5209 - val_acc: 0.7816\n",
      "Epoch 29/1000\n",
      "12864/13096 [============================>.] - ETA: 0s - loss: 0.6429 - acc: 0.7746\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "13096/13096 [==============================] - 3s 205us/sample - loss: 0.6394 - acc: 0.7747 - val_loss: 0.5178 - val_acc: 0.7795\n",
      "Epoch 30/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.4675 - acc: 0.7851 - val_loss: 0.5105 - val_acc: 0.7864\n",
      "Epoch 31/1000\n",
      "13096/13096 [==============================] - 2s 167us/sample - loss: 0.4651 - acc: 0.7914 - val_loss: 0.4986 - val_acc: 0.7871\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc611235f8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42, test_size=0.1)\n",
    "\n",
    "model = load_model('transfer_learning.h5')\n",
    "out = Dense(1, name='ouput')(model.layers[-2].output)\n",
    "model2 = Model(model.input, out)\n",
    "\n",
    "for x in range(4):\n",
    "    model2.layers[x].trainable = False\n",
    "\n",
    "model2.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=1)\n",
    "callbacks_list = [early, redonplat]  # early\n",
    "\n",
    "model2.summary()\n",
    "model2.fit(X_train, Y_train, epochs=1000, verbose=1, callbacks=callbacks_list, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 186, 1)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                8960      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "ouput (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 17,345\n",
      "Trainable params: 17,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13096 samples, validate on 1456 samples\n",
      "Epoch 1/1000\n",
      "13096/13096 [==============================] - 5s 363us/sample - loss: 0.5468 - acc: 0.8530 - val_loss: 0.6025 - val_acc: 0.8379\n",
      "Epoch 2/1000\n",
      "13096/13096 [==============================] - 4s 300us/sample - loss: 0.5075 - acc: 0.8532 - val_loss: 0.4366 - val_acc: 0.8853\n",
      "Epoch 3/1000\n",
      "13096/13096 [==============================] - 4s 301us/sample - loss: 0.7121 - acc: 0.8550 - val_loss: 0.8267 - val_acc: 0.7891\n",
      "Epoch 4/1000\n",
      "13096/13096 [==============================] - 4s 304us/sample - loss: 0.5582 - acc: 0.7989 - val_loss: 0.4161 - val_acc: 0.7981\n",
      "Epoch 5/1000\n",
      "13056/13096 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8235\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "13096/13096 [==============================] - 5s 345us/sample - loss: 0.3679 - acc: 0.8237 - val_loss: 0.3442 - val_acc: 0.8372\n",
      "Epoch 6/1000\n",
      "13096/13096 [==============================] - 4s 304us/sample - loss: 0.3281 - acc: 0.8513 - val_loss: 0.3469 - val_acc: 0.8475\n",
      "Epoch 7/1000\n",
      "13096/13096 [==============================] - 4s 304us/sample - loss: 0.3259 - acc: 0.8542 - val_loss: 0.3523 - val_acc: 0.8434\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc60271cf8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in range(4):\n",
    "    model2.layers[x].trainable = True\n",
    "    \n",
    "model2.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "model2.summary()\n",
    "model2.fit(X_train, Y_train, epochs=1000, verbose=1, callbacks=callbacks_list, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
