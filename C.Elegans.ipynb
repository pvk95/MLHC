{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Elegans DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the C.Elegens .csv file. We add our own headers - labels stands for whether there is a splice site or not and the DNA is a string representing the DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('exercise_data/C_elegans_acc_seq.csv', header=None, names=['labels', 'DNA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the Test-Train-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(28)\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Make a copy of the raw dna sequences for later use with Shogun\n",
    "train_raw = np.array(train)\n",
    "test_raw = np.array(test)\n",
    "X_train = train_raw[:,1]\n",
    "y_train = train_raw[:,0]\n",
    "X_test = test_raw[:,1]\n",
    "y_test = test_raw[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.090909090909092\n",
      "8.977272727272727\n",
      "9.545454545454545\n"
     ]
    }
   ],
   "source": [
    "# Check the label proportions are similar. stratify=True for splitting threw a weird exception\n",
    "print(100*np.sum(df['labels']==1)/df.shape[0])\n",
    "print(100*np.sum(train['labels']==1)/train.shape[0])\n",
    "print(100*np.sum(test['labels']==1)/test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping DNA to a vector\n",
    "\n",
    "We will map the DNA into a vector, by mapping each Character (A,T,C,G) into a one-hot vector and then concatenating all these vectors together. As we have a string of 82 Characters this gives us a final vector of length 328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import utility\n",
    "train['DNA'] = train['DNA'].map(utility.map_dna_into_vector)\n",
    "test['DNA'] = test['DNA'].map(utility.map_dna_into_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame for later Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_eval_df = pd.DataFrame(data=[], columns=['Name', 'AUROC', 'AUPRC', 'f1_cv', 'f1_test'])\n",
    "auroc_eval_df = pd.DataFrame(data=[], columns=['Name', 'AUROC', 'AUPRC', 'f1_cv', 'f1_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, params, df, train, test, metric):\n",
    "    # Put Data into a readable Matrix format\n",
    "    train_data = np.vstack(train['DNA'].values)\n",
    "    test_data  = np.vstack(test['DNA'].values)\n",
    "    \n",
    "    # Create Instance of our Model\n",
    "    m = model()\n",
    "    \n",
    "    # Search for the best params in our model and print the best score\n",
    "    clf = GridSearchCV(m, params, scoring=metric, cv=5, n_jobs=-1)\n",
    "    clf.fit(train_data, train['labels'].values)\n",
    "    print(f\"The best score was: {clf.best_score_}\")\n",
    "    \n",
    "    # Train our best model on the whole train-dataset\n",
    "    best_estimator = model(**clf.best_params_)\n",
    "    best_estimator.fit(train_data, train['labels'].values)\n",
    "    \n",
    "    # Evaluate on the Test set\n",
    "    pred_val = best_estimator.predict(test_data)\n",
    "    true_val = test['labels'].values\n",
    "    auroc, auprc, f1_test = utility.get_scores(true_val, pred_val)\n",
    "    \n",
    "    # Append to our Dataframe\n",
    "    df = df.append({'Name': model.__name__, 'AUROC':auroc, 'AUPRC': auprc, 'f1_cv':clf.best_score_, 'f1_test':f1_test}, ignore_index=True)\n",
    "    return (best_estimator, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1, 10, 100],\n",
    "    #'class_weight': ['balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was: 0.7817207800759418\n",
      "The best score was: 0.9813946282451627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lg_best_estimator, f1_eval_df = evaluate_model(LogisticRegression, params, f1_eval_df, train, test, 'f1')\n",
    "_, auroc_eval_df = evaluate_model(LogisticRegression, params, auroc_eval_df, train, test, 'roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'kernel': ['linear', 'rbf', 'poly'],\n",
    "          'C': [1, 10, 100],\n",
    "         # 'class_weight': ['balanced'],\n",
    "          'gamma': ['auto', 'scale']\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was: 0.7314899728725189\n",
      "The best score was: 0.9835096896668676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "svc_best_estimator, f1_eval_df = evaluate_model(SVC, params, f1_eval_df, train, test, 'f1')\n",
    "_, auroc_eval_df = evaluate_model(SVC, params, auroc_eval_df, train, test, 'roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators':[10, 100, 300],    \n",
    "   # 'class_weight': ['balanced', 'balanced_subsample']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was: 0.2974020371617215\n",
      "The best score was: 0.9795150774483892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was: 0.9757707661004833\n"
     ]
    }
   ],
   "source": [
    "rfc_best_estimator, f1_eval_df = evaluate_model(RandomForestClassifier, params, f1_eval_df, train, test, 'f1')\n",
    "_, auroc_eval_df = evaluate_model(RandomForestClassifier, params, auroc_eval_df, train, test, 'roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM + String kernels (using SHOGUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import shogun as sg\n",
    "\n",
    "features_train = sg.StringCharFeatures(list(X_train), sg.DNA)\n",
    "labels_train = sg.BinaryLabels(y_train.astype(int))\n",
    "features_test = sg.StringCharFeatures(list(X_test), sg.DNA)\n",
    "labels_test = sg.BinaryLabels(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Degree Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform optimization via grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root\n",
    "#param_tree_root = sg.ModelSelectionParameters()\n",
    "\n",
    "#Parameter C\n",
    "#C = sg.ModelSelectionParameters(\"C\")\n",
    "#param_tree_root.append_child(C)\n",
    "#C.build_values(1, 10, sg.R_LINEAR, 1, 2)\n",
    "#C.set_values(1,2,3)\n",
    "\n",
    "#kernel = sg.WeightedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "#svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "#C.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score F1Measure 0.8056770322238286\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "\n",
    "kernel = sg.WeightedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(25)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred_val = svm.apply(features_test).get_labels()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'WDK_' + str(kernel_degree), 'AUROC':auroc, 'AUPRC': auprc, 'f1_cv':cv_score, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Degree String Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score F1Measure 0.7802076269985179\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "\n",
    "kernel = sg.FixedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(25)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred_val = svm.apply(features_test).get_labels()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'FDK_' + str(kernel_degree), 'AUROC':auroc, 'AUPRC': auprc, 'f1_cv':cv_score, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oligo String Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score F1Measure 0.7418407239375421\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "kernel_width = 10\n",
    "\n",
    "kernel = sg.OligoStringKernel(features_train, features_train, kernel_degree, kernel_width)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(1)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was: 0.7282852086438152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was: 0.9757707661004833\n"
     ]
    }
   ],
   "source": [
    "gpc_best_estimator, f1_eval_df = evaluate_model(GaussianProcessClassifier, params, f1_eval_df, train, test, 'f1')\n",
    "_, auroc_eval_df = evaluate_model(GaussianProcessClassifier, params, auroc_eval_df, train, test, 'roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM + String kernels (using SHOGUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shogun'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8415a31c861f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshogun\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringCharFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringCharFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shogun'"
     ]
    }
   ],
   "source": [
    "import shogun as sg\n",
    "\n",
    "features_train = sg.StringCharFeatures(list(X_train), sg.DNA)\n",
    "labels_train = sg.BinaryLabels(y_train.astype(int))\n",
    "features_test = sg.StringCharFeatures(list(X_test), sg.DNA)\n",
    "labels_test = sg.BinaryLabels(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Degree Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform optimization via grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root\n",
    "#param_tree_root = sg.ModelSelectionParameters()\n",
    "\n",
    "#Parameter C\n",
    "#C = sg.ModelSelectionParameters(\"C\")\n",
    "#param_tree_root.append_child(C)\n",
    "#C.build_values(1, 10, sg.R_LINEAR, 1, 2)\n",
    "#C.set_values(1,2,3)\n",
    "\n",
    "#kernel = sg.WeightedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "#svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "#C.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score F1Measure 0.8056770322238286\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "\n",
    "kernel = sg.WeightedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(25)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred_val = svm.apply(features_test).get_labels()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'WDK_' + str(kernel_degree), 'AUROC':auroc, 'AUPRC': auprc, 'f1_cv':cv_score, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Degree String Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score F1Measure 0.7802076269985179\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "\n",
    "kernel = sg.FixedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(25)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred_val = svm.apply(features_test).get_labels()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'FDK_' + str(kernel_degree), 'AUROC':auroc, 'AUPRC': auprc, 'f1_cv':cv_score, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oligo String Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score F1Measure 0.7418407239375421\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "kernel_width = 10\n",
    "\n",
    "kernel = sg.OligoStringKernel(features_train, features_train, kernel_degree, kernel_width)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(1)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred_val = svm.apply(features_test).get_labels()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'OSK_' + str(kernel_degree) + '_' + str(kernel_width),\n",
    "                                'AUROC':auroc, 'AUPRC': auprc, 'f1_cv':cv_score, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Degree Position String Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score F1Measure 0.7844177775424716\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "kernel_degree = 1\n",
    "\n",
    "kernel = sg.WeightedDegreePositionStringKernel(features_train, features_train, kernel_degree)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(25)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred_val = svm.apply(features_test).get_labels()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'WDPSK_' + str(kernel_degree),\n",
    "                                'AUROC':auroc, 'AUPRC': auprc, 'f1_cv':cv_score, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import BatchNormalization,Conv1D,Input,Add,Dense,Flatten\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, \"int32\")\n",
    "    y_pred = tf.cast(tf.round(y_pred), \"int32\") # implicit 0.5 threshold via tf.round\n",
    "    y_correct = tf.cast(tf.equal(y_true,y_pred),\"int32\")\n",
    "    sum_true = tf.reduce_sum(y_true, axis=1)\n",
    "    sum_pred = tf.reduce_sum(y_pred, axis=1)\n",
    "    sum_correct = tf.reduce_sum(y_correct, axis=1)\n",
    "    precision = sum_correct / sum_pred\n",
    "    recall = sum_correct / sum_true\n",
    "    f_score = 2 * precision * recall / (1 * precision + recall)\n",
    "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
    "    return tf.reduce_mean(f_score)\n",
    "\n",
    "def add_RB(x):\n",
    "    xout=BatchNormalization()(x)\n",
    "    xout=Conv1D(filters=32,kernel_size=11,dilation_rate=1,padding='same',activation='relu')(x)\n",
    "    xout=BatchNormalization()(xout)\n",
    "    xout=Conv1D(filters=32,kernel_size=11,dilation_rate=1,padding='same',activation='relu')(xout)\n",
    "    return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1760/1760 [==============================] - 3s 2ms/step - loss: 0.5781 - f1: 0.0807\n",
      "Epoch 2/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.3274 - f1: 0.0830\n",
      "Epoch 3/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.1549 - f1: 0.0875\n",
      "Epoch 4/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.1009 - f1: 0.0881\n",
      "Epoch 5/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0685 - f1: 0.0892\n",
      "Epoch 6/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0510 - f1: 0.0898\n",
      "Epoch 7/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0450 - f1: 0.0898\n",
      "Epoch 8/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0315 - f1: 0.0898\n",
      "Epoch 9/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0258 - f1: 0.0898\n",
      "Epoch 10/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0225 - f1: 0.0898\n",
      "Epoch 11/20\n",
      "1760/1760 [==============================] - 3s 1ms/step - loss: 0.0184 - f1: 0.0898\n",
      "Epoch 12/20\n",
      "1760/1760 [==============================] - 3s 2ms/step - loss: 0.0157 - f1: 0.0898\n",
      "Epoch 13/20\n",
      "1760/1760 [==============================] - 3s 1ms/step - loss: 0.0115 - f1: 0.0898\n",
      "Epoch 14/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0080 - f1: 0.0898\n",
      "Epoch 15/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0090 - f1: 0.0898\n",
      "Epoch 16/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0083 - f1: 0.0898\n",
      "Epoch 17/20\n",
      "1760/1760 [==============================] - 3s 2ms/step - loss: 0.0101 - f1: 0.0898\n",
      "Epoch 18/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0061 - f1: 0.0898\n",
      "Epoch 19/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0055 - f1: 0.0898\n",
      "Epoch 20/20\n",
      "1760/1760 [==============================] - 2s 1ms/step - loss: 0.0044 - f1: 0.0898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/pandas/core/generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Users/karthik/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x=Input(shape=[328,1])\n",
    "\n",
    "x1=Conv1D(filters=32,kernel_size=1,dilation_rate=1,padding='same',activation='relu')(x)\n",
    "\n",
    "xrb=add_RB(x1)\n",
    "#xrb=add_RB(xrb)\n",
    "#xrb=add_RB(xrb)\n",
    "#xrb=add_RB(xrb)\n",
    "\n",
    "x2=Conv1D(filters=32,kernel_size=1,dilation_rate=1,padding='same',activation='relu')(xrb)\n",
    "x3=Conv1D(filters=32,kernel_size=1,dilation_rate=1,padding='same',activation='relu')(x1)\n",
    "\n",
    "xout=Conv1D(filters=1,kernel_size=1,dilation_rate=1,padding='same',activation='relu')(Add()([x2,x3]))\n",
    "xout=Flatten()(xout)\n",
    "xout=Dense(units=1,activation='sigmoid')(xout)\n",
    "\n",
    "model=Model(x,xout)\n",
    "model.compile(optimizer=Adam(),loss='binary_crossentropy',metrics=[f1])\n",
    "class_wt={0:1,1:15}\n",
    "\n",
    "train_data = np.vstack(train['DNA'].values)[:,:,None]\n",
    "test_data  = np.vstack(test['DNA'].values)[:,:,None]\n",
    "\n",
    "train_val=train['labels'].values\n",
    "train_val[train_val==-1]=0\n",
    "model.fit(x=train_data,y=train_val,batch_size=64,epochs=20,class_weight=class_wt)\n",
    "\n",
    "pred_val=(model.predict(test_data)>0.5).astype(np.int)\n",
    "true_val=test['labels']\n",
    "true_val[true_val==-1]=0\n",
    "dl_mtr=utility.get_scores(true_val,pred_val)\n",
    "dl_f1=utility.f1_score(true_val,pred_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_eval_df = f1_eval_df.append({'Name': 'DL Model',\n",
    "                                'AUROC':dl_mtr[0], 'AUPRC': dl_mtr[1], 'f1_cv':'Nill', 'f1_test':dl_f1}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "      <th>f1_cv</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869646</td>\n",
       "      <td>0.782560</td>\n",
       "      <td>0.781721</td>\n",
       "      <td>0.771084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.882807</td>\n",
       "      <td>0.805523</td>\n",
       "      <td>0.73149</td>\n",
       "      <td>0.795181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.570172</td>\n",
       "      <td>0.540909</td>\n",
       "      <td>0.297402</td>\n",
       "      <td>0.244898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "      <td>0.868390</td>\n",
       "      <td>0.773268</td>\n",
       "      <td>0.728285</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "      <td>0.868390</td>\n",
       "      <td>0.773268</td>\n",
       "      <td>0.728285</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DL Model</td>\n",
       "      <td>0.911642</td>\n",
       "      <td>0.873339</td>\n",
       "      <td>Nill</td>\n",
       "      <td>0.864198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name     AUROC     AUPRC     f1_cv   f1_test\n",
       "0         LogisticRegression  0.869646  0.782560  0.781721  0.771084\n",
       "1                        SVC  0.882807  0.805523   0.73149  0.795181\n",
       "2     RandomForestClassifier  0.570172  0.540909  0.297402  0.244898\n",
       "3  GaussianProcessClassifier  0.868390  0.773268  0.728285  0.761905\n",
       "4  GaussianProcessClassifier  0.868390  0.773268  0.728285  0.761905\n",
       "5                   DL Model  0.911642  0.873339      Nill  0.864198"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "      <th>f1_cv</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869646</td>\n",
       "      <td>0.782560</td>\n",
       "      <td>0.981395</td>\n",
       "      <td>0.771084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.547727</td>\n",
       "      <td>0.983510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.633874</td>\n",
       "      <td>0.979515</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "      <td>0.868390</td>\n",
       "      <td>0.773268</td>\n",
       "      <td>0.975771</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "      <td>0.868390</td>\n",
       "      <td>0.773268</td>\n",
       "      <td>0.975771</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name     AUROC     AUPRC     f1_cv   f1_test\n",
       "0         LogisticRegression  0.869646  0.782560  0.981395  0.771084\n",
       "1                        SVC  0.500000  0.547727  0.983510  0.000000\n",
       "2     RandomForestClassifier  0.595238  0.633874  0.979515  0.320000\n",
       "3  GaussianProcessClassifier  0.868390  0.773268  0.975771  0.761905\n",
       "4  GaussianProcessClassifier  0.868390  0.773268  0.975771  0.761905"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
