{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z1GAmYOwsp-"
   },
   "source": [
    "# Prostate image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHHiGzSr0U2Q"
   },
   "source": [
    "We approached the task by implementing a model based on UNET. The 3d images were spliced across their depth and considered as independent samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmXfc5gsyniN"
   },
   "outputs": [],
   "source": [
    "# source_path = 'data/'\n",
    "source_path = 'exercise_data/project4_data/data/'\n",
    "augmentation_factor = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CvqqRYnkMuW"
   },
   "source": [
    "## Data utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eaHda67yjuHh"
   },
   "outputs": [],
   "source": [
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def fix_shape(im):\n",
    "    \"\"\"\n",
    "    Adapt image size to fit UNET\n",
    "    \"\"\"\n",
    "    im = np.transpose(im, axes=[1, 2, 0])\n",
    "    imr = cv2.resize(im, (256, 256), cv2.INTER_CUBIC)\n",
    "    imr = np.transpose(imr, axes=(2, 0, 1))\n",
    "    return imr\n",
    "\n",
    "def disc_labels(labels):\n",
    "    \"\"\"\n",
    "    One hot encoder\n",
    "    \"\"\"\n",
    "    n_samples = labels.shape[0]\n",
    "    height = labels.shape[1]\n",
    "    width = labels.shape[2]\n",
    "    n_labels = 3\n",
    "    labels_disc = np.zeros(shape=(n_samples,height,width,n_labels))\n",
    "\n",
    "    for samp in range(n_samples):\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                pos = int(labels[samp,i,j])\n",
    "                labels_disc[samp,i,j,pos] = 1\n",
    "    return labels_disc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8nv6zeTxL9e"
   },
   "outputs": [],
   "source": [
    "def get_data(source_path):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "    test_images = []\n",
    "    test_labels_rot = []\n",
    "    test_images_rot = []\n",
    "\n",
    "    # train images\n",
    "    for x in range(50):\n",
    "        train_image_path = os.path.join(source_path, f'train_images/sample-{x}.npy')\n",
    "        train_label_path = os.path.join(source_path, f'train_labels/sample-{x}.npy')\n",
    "        mri_image = np.load(train_image_path)\n",
    "        mri_label = np.load(train_label_path)\n",
    "\n",
    "        train_images.append(mri_image)\n",
    "        train_labels.append(mri_label)\n",
    "\n",
    "    #test images\n",
    "    for x in range(50,60):\n",
    "        test_image_path = os.path.join(source_path, f'test_images/sample-{x}.npy')\n",
    "        test_image_rot_path = os.path.join(source_path, f'test_images_randomly_rotated/sample-{x}.npy')\n",
    "        test_label_path = os.path.join(source_path, f'test_labels/sample-{x}.npy')\n",
    "        test_label_rot_path = os.path.join(source_path, f'test_labels_randomly_rotated/sample-{x}.npy')\n",
    "        test_mri_image = np.load(test_image_path)\n",
    "        test_mri_rot = np.load(test_image_rot_path)\n",
    "        test_label = np.load(test_label_path)\n",
    "        test_label_rot = np.load(test_label_rot_path)\n",
    "        if x==52:\n",
    "            test_mri_image = fix_shape(test_mri_image)\n",
    "            test_mri_rot = fix_shape(test_mri_rot)\n",
    "            test_label = fix_shape(test_label)\n",
    "            test_label_rot = fix_shape(test_label_rot)\n",
    "\n",
    "        test_images.append(test_mri_image)\n",
    "        test_labels.append(test_label)\n",
    "        test_images_rot.append(test_mri_rot)\n",
    "        test_labels_rot.append(test_label_rot)\n",
    "\n",
    "    return train_images, train_labels, \\\n",
    "           test_images, test_images_rot, \\\n",
    "           test_labels, test_labels_rot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgW6P_hZ0JLS"
   },
   "source": [
    "We augmented the training dataset by applying random shearing and rotation to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVj3LZU00A75"
   },
   "outputs": [],
   "source": [
    "def shear_img(img, label):\n",
    "    shear_angle = np.random.random() - 0.5\n",
    "    rot_angle = np.random.random() * 2 * np.pi\n",
    "    transform = AffineTransform(shear=shear_angle, rotation=rot_angle)\n",
    "    img = warp(img, transform)\n",
    "    label = warp(img, transform)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def apply_transformation(X, Y, transform, subsample):\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "    for img, label in zip(X, Y):\n",
    "        new_X.append(img)\n",
    "        new_Y.append(label)\n",
    "        for _ in range(subsample):\n",
    "            new_img, new_label = transform(img, label)\n",
    "            new_X.append(new_img)\n",
    "            new_Y.append(new_label)\n",
    "    return new_X, new_Y\n",
    "\n",
    "\n",
    "def augment_data(X, Y, augmentation_factor):\n",
    "    np.random.seed(10)\n",
    "    X, Y = apply_transformation(X, Y, shear_img, augmentation_factor)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELBTOQTC38nY"
   },
   "source": [
    "We also cropped each image in 9 overlapping 128x128 pieces. Resolution of labeling after reconstruction was performed taking the median prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzxZiBRF39Sd"
   },
   "outputs": [],
   "source": [
    "def sliding_window(image):\n",
    "    images = []\n",
    "    for y in range(3):\n",
    "        for x in range(3):\n",
    "            img = image[x*64:128+x*64,y*64:128+y*64]\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "def sub_images(images):\n",
    "    sub_images = []\n",
    "    for image in images:\n",
    "        imgs = sliding_window(image)\n",
    "        sub_images.append(imgs)\n",
    "    sub_images = np.concatenate(sub_images)\n",
    "    return sub_images\n",
    "\n",
    "def reconstruct_image_sum(sub_images):\n",
    "    image = np.zeros((256, 256, 3))\n",
    "    for y in range(3):\n",
    "        for x in range(3):\n",
    "            idx = x + 3*y\n",
    "            image[x*64: 128+x*64, y*64: 128+64*y,:] += sub_images[idx]\n",
    "    return image\n",
    "\n",
    "def reconstruct_image_median(sub_images):\n",
    "    image = np.empty((256,256,3,9))\n",
    "    image.fill(np.nan)\n",
    "    for y in range(3):\n",
    "        for x in range(3):\n",
    "            idx = x + 3*y\n",
    "            image[x*64: 128+x*64, y*64: 128+64*y, :, idx] = sub_images[idx]\n",
    "    image = np.nanmedian(image, 3)\n",
    "    return image\n",
    "\n",
    "def reconstruct_array(array):\n",
    "    images = []\n",
    "    length = array.shape[0]\n",
    "    splits = length //9\n",
    "    for imgs in np.split(array, splits):\n",
    "        img = reconstruct_image_median(imgs)\n",
    "        images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDegFTtO6aow"
   },
   "source": [
    "## Metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMP1ihDy6bei"
   },
   "outputs": [],
   "source": [
    "def getMetrics(gt,pred):\n",
    "\n",
    "    n_labels = 3\n",
    "    conf = np.zeros(shape=(n_labels,n_labels))\n",
    "\n",
    "    assert len(gt.shape) ==3\n",
    "    assert len(pred.shape) == 3\n",
    "\n",
    "    n_samples = gt.shape[0]\n",
    "    height = gt.shape[1]\n",
    "    width = gt.shape[2]\n",
    "\n",
    "    for sample in range(n_samples):\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                obj_gt = gt[sample,i,j]\n",
    "                obj_pd = pred[sample,i,j]\n",
    "                conf[obj_gt,obj_pd] +=1\n",
    "                conf[obj_pd, obj_gt] += 1\n",
    "\n",
    "    op = np.trace(conf)/np.sum(conf)\n",
    "\n",
    "    pc = np.sum(np.diag(conf)/np.sum(conf,axis=-1))/n_labels\n",
    "\n",
    "    iou = np.sum(np.diag(conf)/(np.sum(conf,axis=-1) + np.sum(conf,axis=0) - np.diag(conf)))/n_labels\n",
    "\n",
    "    return op,pc,iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzZyNuvhkVRQ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lvt9_jyu60c3"
   },
   "source": [
    "As already mentioned, the proposed model is based on UNET, the main difference being the inclusion of padding in the convolutional layers and allowing for variable depth. We experimented training with dice loss apart from cross entropy without noting great differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btyrQ_LZkV6m"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "class UNet(object):\n",
    "    def __init__(self,save_folder,input_shape=(400, 400, 3), epochs=30, verbose=1, batch_size=4, deepness=4):\n",
    "        self.input_shape = input_shape\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.deepness = deepness\n",
    "        self.save_folder = save_folder\n",
    "\n",
    "    def create_model(self):\n",
    "        self.input = keras.layers.Input(self.input_shape)\n",
    "        self.skip = []\n",
    "        inp = self.input\n",
    "        # Convolution\n",
    "        for x in range(self.deepness):\n",
    "            filters = 2**(6 + x)\n",
    "            skip, inp = self.conv_layer(filters, inp)\n",
    "            self.skip.append(skip)\n",
    "\n",
    "        # lowest layer\n",
    "        conv1 = keras.layers.Conv2D(\n",
    "            2**(6 + self.deepness), 3, activation='relu', padding='same')(inp)\n",
    "        conv2 = keras.layers.Conv2D(\n",
    "            2**(6 + self.deepness), 3, activation='relu', padding='same')(conv1)\n",
    "\n",
    "        # Upsample and convolutions\n",
    "        inp = conv2\n",
    "        for x in range(self.deepness - 1, -1, -1):\n",
    "            filters = 2**(6 + x)\n",
    "            inp = self.upconv_layer(filters, inp, self.skip[x])\n",
    "\n",
    "        output = keras.layers.Conv2D(3, 1, activation='softmax')(inp)\n",
    "        model = keras.models.Model(inputs=self.input, outputs=output)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def conv_layer(self, filters, inp):\n",
    "        conv1 = keras.layers.Conv2D(\n",
    "            filters, 3, activation='relu', padding='same')(inp)\n",
    "        conv2 = keras.layers.Conv2D(\n",
    "            filters, 3, activation='relu', padding='same')(conv1)\n",
    "        max_pool = keras.layers.MaxPool2D(2, strides=2)(conv2)\n",
    "        return conv2, max_pool\n",
    "\n",
    "    def upconv_layer(self, filters, inp, skip):\n",
    "        up_conv = keras.layers.Conv2DTranspose(filters, 2, 2)(inp)\n",
    "        up_shape = up_conv.shape.as_list()\n",
    "        skip_shape = skip.shape.as_list()\n",
    "\n",
    "        x_start = (skip_shape[1] - up_shape[1]) // 2\n",
    "        y_start = (skip_shape[2] - up_shape[2]) // 2\n",
    "        x_end = x_start + up_shape[1]\n",
    "        y_end = y_start + up_shape[2]\n",
    "\n",
    "        cut_skip = keras.layers.Lambda(\n",
    "            lambda x: x[:, x_start:x_end, y_start: y_end, :])(skip)\n",
    "\n",
    "        merge = keras.layers.concatenate([cut_skip, up_conv], axis=-1)\n",
    "        conv1 = keras.layers.Conv2D(\n",
    "            filters, 3, activation='relu', padding='same')(merge)\n",
    "        conv2 = keras.layers.Conv2D(\n",
    "            filters, 3, activation='relu', padding='same')(conv1)\n",
    "\n",
    "        return conv2\n",
    "\n",
    "    def fit(self, X, y, validation_data=None):\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=self.verbose)\n",
    "        redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=self.verbose)\n",
    "        self.model = self.create_model()\n",
    "        self.model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                           loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        if not validation_data:\n",
    "            self.model.fit(x=X, y=y, batch_size=self.batch_size, verbose=self.verbose,\n",
    "                        validation_split=0.1, epochs=self.epochs, callbacks=[early, redonplat])\n",
    "        else:\n",
    "            self.model.fit(x=X, y=y, batch_size=self.batch_size, verbose=self.verbose,\n",
    "                        validation_data=validation_data, epochs=self.epochs, callbacks=[early, redonplat])\n",
    "\n",
    "        if not os.path.exists(self.save_folder + 'checkpoint/'):\n",
    "            os.makedirs(self.save_folder + 'checkpoint/')\n",
    "        tf.keras.models.save_model(self.model,self.save_folder + 'checkpoint/Unet.h5')\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        fileName = self.save_folder + 'checkpoint/Unet.h5'\n",
    "        if not os.path.isfile(fileName):\n",
    "            print(\"Model not found! Exiting ...\")\n",
    "            sys.exit(1)\n",
    "        self.model = tf.keras.models.load_model(fileName)\n",
    "        y_pred = self.model.predict(X, batch_size=self.batch_size)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'input_shape': self.input_shape,\n",
    "            'epochs': self.epochs,\n",
    "            'verbose': self.verbose,\n",
    "            'batch_size': self.batch_size,\n",
    "            'deepness': self.deepness\n",
    "        }\n",
    "\n",
    "    def set_params(self, **paramters):\n",
    "        for paramter, value in paramters.items():\n",
    "            setattr(self, paramter, value)\n",
    "\n",
    "    def train(self, X_train, Y_train, validation_data=None):\n",
    "        self.model = self.create_model()\n",
    "        self.model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                           loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        if not validation_data:\n",
    "            history = self.model.fit(x=X_train, y=Y_train, validation_split=0.1,\n",
    "                        batch_size=self.batch_size, verbose=self.verbose, epochs=self.epochs)\n",
    "        else:\n",
    "            history = self.model.fit(x=X_train, y=Y_train, validation_data=validation_data,\n",
    "                        batch_size=self.batch_size, verbose=self.verbose, epochs=self.epochs)\n",
    "\n",
    "        training_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        train_curves = {'train': training_loss, 'val': val_loss}\n",
    "\n",
    "        with open(self.save_folder + 'train_curves.pickle', 'wb') as f:\n",
    "            pickle.dump(train_curves, f)\n",
    "\n",
    "        if not os.path.exists(self.save_folder + 'checkpoint/'):\n",
    "            os.makedirs(self.save_folder + 'checkpoint')\n",
    "\n",
    "        fileName = self.save_folder + 'checkpoint/UNet.h5'\n",
    "        tf.keras.models.save_model(self.model,filepath=fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJrjXt5BkmEx"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCvX_FC_kmkL"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "train_images, train_labels, test_images, test_images_rot, test_labels, test_labels_rot = get_data(source_path)\n",
    "\n",
    "train_images = np.concatenate(train_images)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(\n",
    "    train_images, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# train_X, train_Y = augment_data(train_X, train_Y, augmentation_factor)\n",
    "\n",
    "sub_train_X = sub_images(train_X)\n",
    "sub_train_Y = sub_images(train_Y)\n",
    "sub_val_X = sub_images(val_X)\n",
    "sub_val_Y = sub_images(val_Y)\n",
    "\n",
    "\n",
    "sub_train_Y = disc_labels(sub_train_Y)\n",
    "sub_val_Y = disc_labels(sub_val_Y)\n",
    "\n",
    "sub_train_X = sub_train_X[..., np.newaxis]\n",
    "sub_val_X = sub_val_X[..., np.newaxis]\n",
    "\n",
    "sub_train_X, sub_train_Y = shuffle(sub_train_X, sub_train_Y)\n",
    "\n",
    "unet = UNet('./', input_shape=(128, 128, 1),\n",
    "                   epochs=100, deepness=3, verbose=1, batch_size=32)\n",
    "#unet.fit(sub_train_X, sub_train_Y, (sub_val_X, sub_val_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jB82nFwr_oHF"
   },
   "source": [
    "#### Quantitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26336,
     "status": "ok",
     "timestamp": 1558555119251,
     "user": {
      "displayName": "Juan Lopez",
      "photoUrl": "",
      "userId": "12219960520819140406"
     },
     "user_tz": -120
    },
    "id": "iXMcMn7y_kFA",
    "outputId": "fed02a09-db56-4d5d-e7ab-23b25c81c209"
   },
   "outputs": [],
   "source": [
    "sub_pred_Y = unet.predict(sub_val_X)\n",
    "pred_Y = reconstruct_array(sub_pred_Y)\n",
    "pred_Y = np.array(pred_Y)\n",
    "pred_Y = np.argmax(pred_Y, axis=-1).astype(int)\n",
    "\n",
    "op, pc, iou = getMetrics(val_Y, pred_Y)\n",
    "curr_metrics = {'OP': op, 'PC': pc, 'IoU': iou}\n",
    "\n",
    "metrics = pd.DataFrame({}, columns=['OP', 'PC', 'IoU'])\n",
    "metrics = metrics.append(curr_metrics, ignore_index=True)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxIcTeXS_u4w"
   },
   "source": [
    "#### Qualitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4375,
     "status": "ok",
     "timestamp": 1558555157132,
     "user": {
      "displayName": "Juan Lopez",
      "photoUrl": "",
      "userId": "12219960520819140406"
     },
     "user_tz": -120
    },
    "id": "hjXbkhKpui0w",
    "outputId": "677d8aea-3e6c-45e4-82b8-9e56662df02e"
   },
   "outputs": [],
   "source": [
    "display_num = 5\n",
    "\n",
    "r_choices = np.random.choice(val_Y.shape[0], display_num)\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(display_num):\n",
    "  img_num = r_choices[i // 2]\n",
    "  \n",
    "  plt.subplot(display_num, 3, 3 * i + 1)\n",
    "  plt.imshow(val_X[img_num, :, :], cmap='gray')\n",
    "  plt.title(\"Input image\")\n",
    "  \n",
    "  plt.subplot(display_num, 3, 3 * i + 2)\n",
    "  plt.imshow(val_Y[img_num, :, :], cmap='gray')\n",
    "  plt.title(\"Actual Mask\")\n",
    "  \n",
    "  plt.subplot(display_num, 3, 3 * i + 3)\n",
    "  plt.imshow(pred_Y[img_num, :, :], cmap='gray')\n",
    "  plt.title(\"Predicted Mask\")\n",
    "\n",
    "plt.suptitle(\"Examples of Input Image, Label, and Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGCDzEfz_DOs"
   },
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OW8xiAu__L7U"
   },
   "source": [
    "### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0-KlGdl_FVi"
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "###      TEST       ###\n",
    "#######################\n",
    "import csv\n",
    "\n",
    "images_name = np.arange(50,60)\n",
    "for i,test_im in enumerate(test_images):\n",
    "    sub_test_im = sub_images(test_im)\n",
    "    sub_test_im = sub_test_im[..., np.newaxis]\n",
    "    sub_pred_im = unet.predict(sub_test_im)\n",
    "    pred_im = reconstruct_array(sub_pred_im)\n",
    "    pred_im = np.argmax(pred_im, axis=-1).astype(int)\n",
    "\n",
    "    op, pc, iou = getMetrics(test_labels[i], pred_im)\n",
    "\n",
    "    with open('test_images.csv', 'a') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(['sample-{}'.format(images_name[i]), op,pc,iou])\n",
    "    curr_metrics = {'OP': op, 'PC': pc, 'IoU': iou}\n",
    "    metrics = metrics.append(curr_metrics, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_csv = pd.read_csv('test_images.csv',delimiter=',',header=None)\n",
    "test_images_csv.columns = ['Sample','OP','PC','IoU']\n",
    "test_images_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0f4tS6Y_OhJ"
   },
   "source": [
    "### Randomly rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtWVGulY_Pzd"
   },
   "outputs": [],
   "source": [
    "images_name = np.arange(50,60)\n",
    "for i,rot_im in enumerate(test_images_rot):\n",
    "    sub_test_rot = sub_images(rot_im)\n",
    "    sub_test_rot = sub_test_rot[..., np.newaxis]\n",
    "    sub_pred_rot = unet.predict(sub_test_rot)\n",
    "    pred_rot = reconstruct_array(sub_pred_rot)\n",
    "    pred_rot = np.argmax(pred_rot, axis=-1).astype(int)\n",
    "\n",
    "    op, pc, iou = getMetrics(test_labels_rot[i], pred_rot)\n",
    "    with open('test_images_randomly_rotated.csv', 'a') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(['sample-{}'.format(images_name[i]), op,pc,iou])\n",
    "    curr_metrics = {'OP': op, 'PC': pc, 'IoU': iou}\n",
    "    metrics = metrics.append(curr_metrics, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_images_csv = pd.read_csv('test_images_randomly_rotated.csv',delimiter=',',header=None)\n",
    "rot_images_csv.columns = ['Sample','OP','PC','IoU']\n",
    "rot_images_csv"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "exercise4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
