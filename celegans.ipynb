{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Elegans DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the C.Elegens .csv file. We add our own headers - labels stands for whether there is a splice site or not and the DNA is a string representing the DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('exercise_data/C_elegans_acc_seq.csv', header=None, names=['labels', 'DNA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the Test-Train-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(28)\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Make a copy of the raw dna sequences for later use with Shogun\n",
    "train_raw = np.array(train)\n",
    "test_raw = np.array(test)\n",
    "X_train = train_raw[:,1]\n",
    "y_train = train_raw[:,0]\n",
    "X_test = test_raw[:,1]\n",
    "y_test = test_raw[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the label proportions are similar. stratify=True for splitting threw a weird exception\n",
    "print(100*np.sum(df['labels']==1)/df.shape[0])\n",
    "print(100*np.sum(train['labels']==1)/train.shape[0])\n",
    "print(100*np.sum(test['labels']==1)/test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping DNA to a vector\n",
    "\n",
    "We will map the DNA into a vector, by mapping each Character (A,T,C,G) into a one-hot vector and then concatenating all these vectors together. As we have a string of 82 Characters this gives us a final vector of length 328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility\n",
    "train['DNA'] = train['DNA'].map(utility.map_dna_into_vector)\n",
    "test['DNA'] = test['DNA'].map(utility.map_dna_into_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame for later Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_eval_df = pd.DataFrame(data=[], columns=['Name', 'AUROC_cv', 'f1_cv', 'AUROC_test', 'AUPRC_test', 'f1_test'])\n",
    "auroc_eval_df = pd.DataFrame(data=[], columns=['Name', 'AUROC_cv', 'f1_cv', 'AUROC_test', 'AUPRC_test', 'f1_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, params, df, train, test, metric, half=0, svc=0):\n",
    "    # Put Data into a readable Matrix format\n",
    "    train_data = np.vstack(train['DNA'].values)\n",
    "    test_data  = np.vstack(test['DNA'].values)\n",
    "    \n",
    "    if half:\n",
    "        if half == 1:\n",
    "            train_data = train_data[:, :int(train_data.shape[1]/2) - 4]\n",
    "            test_data = test_data[:, :int(test_data.shape[1]/2) - 4]\n",
    "        elif half == 2:\n",
    "            train_data = train_data[:, (int(train_data.shape[1]/2) + 8):]\n",
    "            test_data = test_data[:, (int(test_data.shape[1]/2) + 8):]\n",
    "        else:\n",
    "            raise ValueError('half must take values 0|1|2')\n",
    "    \n",
    "    # Create Instance of our Model\n",
    "    m = model()\n",
    "    \n",
    "    # Specify metrics\n",
    "    scoring = {'roc_auc': 'roc_auc', 'f1': 'f1'}\n",
    "    \n",
    "    # Search for the best params in our model and print the best score\n",
    "    clf = GridSearchCV(m, params, scoring=scoring, refit=metric, cv=5, n_jobs=-1)\n",
    "    clf.fit(train_data, train['labels'].values)\n",
    "    print(f\"The best score was: {clf.best_score_}\")\n",
    "    \n",
    "    # Extract cv metric results\n",
    "    results = clf.cv_results_\n",
    "    cv_dict = {'AUROC_cv':results['mean_test_roc_auc'][clf.best_index_],\n",
    "               'f1_cv':results['mean_test_f1'][clf.best_index_]}\n",
    "    \n",
    "    # Train our best model on the whole train-dataset\n",
    "    best_estimator = model(**clf.best_params_)\n",
    "    best_estimator.fit(train_data, train['labels'].values)\n",
    "    \n",
    "    # Evaluate on the Test set\n",
    "    pred_val = best_estimator.predict(test_data)\n",
    "    true_val = test['labels'].values\n",
    "    if svc:\n",
    "        pred_scores = best_estimator.decision_function(test_data)\n",
    "    else:\n",
    "        pred_scores = best_estimator.predict_proba(test_data)[:,1]\n",
    "        \n",
    "    auroc_test, auprc_test, f1_test = utility.get_scores(true_val, pred_val, pred_scores)\n",
    "    test_dict = {'AUROC_test': auroc_test, \n",
    "                 'AUPRC_test': auprc_test, \n",
    "                 'f1_test': f1_test}\n",
    "    \n",
    "    # Append to our Dataframe\n",
    "    new_row = {'Name': model.__name__ + '_half' + str(half)} if half else {'Name': model.__name__}\n",
    "    new_row.update(cv_dict)\n",
    "    new_row.update(test_dict)\n",
    "    df = df.append(new_row, ignore_index=True)\n",
    "    return (best_estimator, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1, 10, 100],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_best_estimator, f1_eval_df = evaluate_model(LogisticRegression, params, f1_eval_df, train, test, 'f1')\n",
    "_, auroc_eval_df = evaluate_model(LogisticRegression, params, auroc_eval_df, train, test, 'roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'kernel': ['linear', 'rbf', 'poly'],\n",
    "          'C': [0.1, 1, 10, 100],\n",
    "          'class_weight': [None, 'balanced'],\n",
    "          'gamma': ['auto', 'scale']\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best_estimator, f1_eval_df = evaluate_model(SVC, params, f1_eval_df, train, test, 'f1', svc=1)\n",
    "_, auroc_eval_df = evaluate_model(SVC, params, auroc_eval_df, train, test, 'roc_auc', svc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':[10, 50, 100, 500],\n",
    "          'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_best_estimator, f1_eval_df = evaluate_model(RandomForestClassifier, params, f1_eval_df, train, test, 'f1')\n",
    "_, auroc_eval_df = evaluate_model(RandomForestClassifier, params, auroc_eval_df, train, test, 'roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gaussian Process Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, PairwiseKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'kernel' : [RBF(), PairwiseKernel()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpc_best_estimator, f1_eval_df = evaluate_model(GaussianProcessClassifier, params, f1_eval_df, train, test, 'f1')\n",
    "_, auroc_eval_df = evaluate_model(GaussianProcessClassifier, params, auroc_eval_df, train, test, 'roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM + String kernels (using SHOGUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import shogun as sg\n",
    "\n",
    "features_train = sg.StringCharFeatures(list(X_train), sg.DNA)\n",
    "labels_train = sg.BinaryLabels(y_train.astype(int))\n",
    "features_test = sg.StringCharFeatures(list(X_test), sg.DNA)\n",
    "labels_test = sg.BinaryLabels(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Degree Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform optimization via grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root\n",
    "#param_tree_root = sg.ModelSelectionParameters()\n",
    "\n",
    "#Parameter C\n",
    "#C = sg.ModelSelectionParameters(\"C\")\n",
    "#param_tree_root.append_child(C)\n",
    "#C.build_values(1, 10, sg.R_LINEAR, 1, 2)\n",
    "#C.set_values(1,2,3)\n",
    "\n",
    "#kernel = sg.WeightedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "#svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "#C.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "\n",
    "kernel = sg.WeightedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(25)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred = svm.apply(features_test)\n",
    "pred_val = pred.get_labels()\n",
    "pred_scores = pred.get_values()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val, pred_scores)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'WDK_' + str(kernel_degree), 'f1_cv':cv_score, 'AUROC_test':auroc, 'AUPRC_test': auprc, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Degree String Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "\n",
    "kernel = sg.FixedDegreeStringKernel(features_train, features_train, kernel_degree)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(25)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred = svm.apply(features_test)\n",
    "pred_val = pred.get_labels()\n",
    "pred_scores = pred.get_values()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val, pred_scores)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'FDK_' + str(kernel_degree), 'f1_cv':cv_score, 'AUROC_test':auroc, 'AUPRC_test': auprc, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oligo String Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2\n",
    "kernel_degree = 3\n",
    "kernel_width = 10\n",
    "\n",
    "kernel = sg.OligoStringKernel(features_train, features_train, kernel_degree, kernel_width)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(1)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred = svm.apply(features_test)\n",
    "pred_val = pred.get_labels()\n",
    "pred_scores = pred.get_values()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val, pred_scores)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'OSK_' + str(kernel_degree) + '_' + str(kernel_width),\n",
    "                                'f1_cv':cv_score, 'AUROC_test':auroc, 'AUPRC_test': auprc, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Degree Position String Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2\n",
    "kernel_degree = 1\n",
    "\n",
    "kernel = sg.WeightedDegreePositionStringKernel(features_train, features_train, kernel_degree)\n",
    "svm = sg.LibSVM(C, kernel, labels_train)\n",
    "\n",
    "# Cross validation\n",
    "stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "metric = sg.F1Measure()\n",
    "cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "# 25 runs and 95% confidence intervals\n",
    "cross.set_num_runs(25)\n",
    "cross.set_autolock(False)\n",
    "result = cross.evaluate()\n",
    "cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.train()\n",
    "pred = svm.apply(features_test)\n",
    "pred_val = pred.get_labels()\n",
    "pred_scores = pred.get_values()\n",
    "auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val, pred_scores)\n",
    "    \n",
    "# Append to our Dataframe\n",
    "f1_eval_df = f1_eval_df.append({'Name': 'WDPSK_' + str(kernel_degree), 'f1_cv':cv_score, 'AUROC_test':auroc, 'AUPRC_test': auprc, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Kernel Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combine the better performing previous kernels mixed via a weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    C = 1\n",
    "\n",
    "    #poly_kernel = sg.PolyKernel(dot_features, dot_features, 3, True, 10)\n",
    "    WD_kernel = sg.WeightedDegreeStringKernel(3)\n",
    "    #WDP_kernel = sg.WeightedDegreePositionStringKernel()\n",
    "\n",
    "    combined_kernel = sg.CombinedKernel()\n",
    "    combined_kernel.append_kernel(WD_kernel)\n",
    "    #combined_kernel.append_kernel(WDP_kernel)\n",
    "    combined_kernel.init(features_train, features_train)\n",
    "\n",
    "    svm = sg.LibSVM(C, combined_kernel, labels_train)\n",
    "\n",
    "    # Cross validation\n",
    "    stratified_split = sg.StratifiedCrossValidationSplitting(labels_train, 5)\n",
    "    metric = sg.F1Measure()\n",
    "    cross = sg.CrossValidation(svm, features_train, labels_train, stratified_split, metric)\n",
    "    # 25 runs and 95% confidence intervals\n",
    "    cross.set_num_runs(25)\n",
    "    cross.set_autolock(False)\n",
    "    result = cross.evaluate()\n",
    "    cv_score = sg.CrossValidationResult.obtain_from_generic(result).get_mean()\n",
    "    print(\"CV score\", metric.get_name(), cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on whole train dataset to evaluate test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    svm.train()\n",
    "    pred = svm.apply(features_test)\n",
    "    pred_val = pred.get_labels()\n",
    "    pred_scores = pred.get_values()\n",
    "    auroc, auprc, f1_test = utility.get_scores(y_test.astype(int), pred_val, pred_scores)\n",
    "\n",
    "    # Append to our Dataframe\n",
    "    f1_eval_df = f1_eval_df.append({'Name': 'MKL', 'f1_cv':cv_score, 'AUROC_test':auroc, 'AUPRC_test': auprc, 'f1_test':f1_test}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import BatchNormalization,Conv1D,Input,Add,Dense,Flatten\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "def add_RB(x):\n",
    "    xout=BatchNormalization()(x)\n",
    "    xout=Conv1D(filters=32,kernel_size=11,dilation_rate=1,padding='same',activation='relu')(x)\n",
    "    xout=BatchNormalization()(xout)\n",
    "    xout=Conv1D(filters=32,kernel_size=11,dilation_rate=1,padding='same',activation='relu')(xout)\n",
    "    return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x=Input(shape=[328,1])\n",
    "\n",
    "x1=Conv1D(filters=32,kernel_size=1,dilation_rate=1,padding='same',activation='relu')(x)\n",
    "\n",
    "xrb=add_RB(x1)\n",
    "\n",
    "x2=Conv1D(filters=32,kernel_size=1,dilation_rate=1,padding='same',activation='relu')(xrb)\n",
    "x3=Conv1D(filters=32,kernel_size=1,dilation_rate=1,padding='same',activation='relu')(x1)\n",
    "\n",
    "xout=Conv1D(filters=1,kernel_size=1,dilation_rate=1,padding='same',activation='relu')(Add()([x2,x3]))\n",
    "xout=Flatten()(xout)\n",
    "xout=Dense(units=1,activation='sigmoid')(xout)\n",
    "\n",
    "model=Model(x,xout)\n",
    "model.compile(optimizer=Adam(),loss='binary_crossentropy')\n",
    "class_wt={0:1,1:15}\n",
    "\n",
    "train_data = np.vstack(train['DNA'].values)[:,:,None]\n",
    "test_data  = np.vstack(test['DNA'].values)[:,:,None]\n",
    "\n",
    "train_val=train['labels'].values\n",
    "train_val[train_val==-1]=0\n",
    "model.fit(x=train_data,y=train_val,batch_size=64,epochs=20,class_weight=class_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores=model.predict(test_data)\n",
    "pred_val=(pred_scores>0.5).astype(np.int)\n",
    "true_val=test['labels']\n",
    "true_val[true_val==-1]=0\n",
    "dl_mtr=utility.get_scores(true_val,pred_val,pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_eval_df = f1_eval_df.append({'Name': 'DL Model', 'AUROC_test':dl_mtr[0], 'AUPRC_test': dl_mtr[1], 'f1_test':dl_mtr[2]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC half string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we only use the first or second half of the dna sequences in order to assess the influence of each substring in the accuracy of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First substring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'kernel': ['linear', 'rbf', 'poly'],\n",
    "          'C': [0.1, 1, 10, 100],\n",
    "          'class_weight': [None, 'balanced'],\n",
    "          'gamma': ['auto', 'scale']\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best_estimator_half, f1_eval_df = evaluate_model(SVC, params, f1_eval_df, train, test, 'f1', half=1, svc=1)\n",
    "_, auroc_eval_df = evaluate_model(SVC, params, auroc_eval_df, train, test, 'roc_auc', half=1, svc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second substring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'kernel': ['linear', 'rbf', 'poly'],\n",
    "          'C': [0.1, 1, 10, 100],\n",
    "          'class_weight': [None, 'balanced'],\n",
    "          'gamma': ['auto', 'scale']\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best_estimator_half, f1_eval_df = evaluate_model(SVC, params, f1_eval_df, train, test, 'f1', half=2, svc=1)\n",
    "_, auroc_eval_df = evaluate_model(SVC, params, auroc_eval_df, train, test, 'roc_auc', half=2, svc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "best_estimator = svc_best_estimator\n",
    "predicted_values = best_estimator.predict(np.vstack(test['DNA'].values))\n",
    "predicted_scores = best_estimator.decision_function(np.vstack(test['DNA'].values))\n",
    "true_values = test['labels']\n",
    "\n",
    "# compute ROC curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(true_values, predicted_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "precision, recall, thresholds_prc = precision_recall_curve(true_values, predicted_scores)\n",
    "\n",
    "# compute precision-recall curve\n",
    "auprc = auc(recall, precision)\n",
    "precision_random, recall_random, thresholds_random = precision_recall_curve(true_values, np.random.rand(len(true_values)))\n",
    "auprc_random = auc(recall_random, precision_random)\n",
    "other_scores_validation = [roc_auc, auprc, auprc_random]\n",
    "\n",
    "# plot curves\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity / False Positive Rate')\n",
    "plt.ylabel('Sensitivity / True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('./models/' + experiment_id + '/' + model_name + '_roc_curve_validation.png')\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "                     lw=lw, label='AUPRC curve (area = %0.2f)' % auprc)\n",
    "plt.plot(recall_random, precision_random, color='navy', linestyle='--',\n",
    "                     lw=lw, label='random AUPRC curve (area = %0.2f)' % auprc_random)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernelized SVM and DL works the best, but not much better than other simpler models such as logistic regression.\n",
    "- String kernels didn't improve over polynomial kernel.\n",
    "- First half of the dna sequences holds most of the relevant information for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
